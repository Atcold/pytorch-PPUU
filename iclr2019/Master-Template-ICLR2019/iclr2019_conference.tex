\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\usepackage{graphicx} % more modern
\usepackage{subfigure}
\usepackage{hhline}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{wrapfig}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{tabularx}


%\title{Driving in Dense Traffic via Model-Predictive Policy Learning with Uncertainty Minimization}
\title{
  Model-Predictive Policy Learning with \\
  Uncertainty Regularization for Driving \\
  in Dense Traffic
}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Mikael Henaff, Alfredo Canziani \& Yann LeCun \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
New York University\\
\texttt{\{mbh305\}@nyu.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\newcommand{\modelnamedrop}{MPUR }
\newcommand{\modelnameil}{MPER }

\maketitle

\begin{abstract}
  Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training.
  In this work, we propose to train a policy while explicitly penalizing the mismatch between these two distributions over a fixed time horizon.
  We do this by unrolling a learned model of the environment dynamics over multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory. This cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on.
%We do this by using a learned model of the environment dynamics which is unrolled for multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory. This cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on.
We propose to measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks.
  We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.
\end{abstract}


\section{Introduction}

In recent years, model-free reinforcement learning methods using deep neural network controllers have proven effective on a wide range of tasks, from playing video or text-based games \citep{mnih15, A3C, NarasimhanKB15} to learning algorithms \citep{Zaremba15} and complex locomotion tasks \citep{Lillicrap2015, ZhangLMUC15}.
However, these methods often require a large number of interactions with the environment in order to learn.
While this is not a problem if the environment is simulated, it can limit the application of these methods in realistic environments where interactions with the environment are slow, expensive or potentially dangerous.
Building a simulator where the agent can safely try out policies without facing real consequences can mitigate this problem, but requires human engineering effort which increases with the complexity of the environment being modeled.


Model-based reinforcement learning approaches try to learn a model of the environment dynamics, and then use this model to plan actions or train a parameterized policy.
A common setting is where an agent alternates between collecting experience by executing actions using its current policy or dynamics model, and then using these experiences to improve its dynamics model.
This approach has been shown empirically to significantly reduce the required number of environment interactions needed to obtain an effective policy or planner \citep{Atkeson1997, PILCO, Nagabandi2017, Chua2018}.
%Theoretical results have also shown the model-based approach to offer improved sample complexity in simple settings \citep{LS-LQR, Recht2018}.

Despite these improvements in sample complexity, there exist settings where even a \textit{single} poor action executed by an agent in a real environment can have consequences which are not acceptable.
At the same time, with data collection becoming increasingly inexpensive, there are many settings where observational data of an environment is abundant.
This suggests a need for algorithms which can learn policies primarily from observational data, which can then perform well in a real environment.
Autonomous driving is an example of such a setting: on one hand, trajectories of human drivers can be easily collected using traffic cameras \citep{NGSIM}, resulting in an abundance of observational data; on the other hand, learning through interaction with the real environment is not a viable solution.


\begin{wrapfigure}{r}{0.3\textwidth}
%  \centering
  \includegraphics[width=0.3\textwidth]{figures/driving/uncertainty_example2-crop.pdf}
  \caption{\label{simple-example}Different models fitted on training points which cover a limited region the function's domain. Models make arbitrary predictions outside of this region.}
\end{wrapfigure}
However, learning policies from purely observational data is challenging because the data may only cover a small region of the space over which it is defined.
If the observational data consists of state-action pairs produced by an expert following an optimal policy, one option is to use imitation learning \citep{Pomerleau91}.
However, passive imitation learning is well-known to suffer from a mismatch between the states seen at training and execution time \citep{Ross2010EfficientRF}, and may need to be augmented by querying an expert \citep{Dagger}.
Another option is to learn a dynamics model from observational data, and then use it to train a policy \citep{Nguyen1989}. However, the dynamics model may make arbitrary predictions outside the domain it was trained on, which may wrongly be associated with low cost (or high reward) as shown in Figure \ref{simple-example}. The policy network may then exploit these errors in the dynamics model and produce actions which lead to wrongly optimistic states.
In the interactive setting, this problem is naturally self-correcting, since states where the model predictions are wrongly optimistic will be more likely to be experienced, and thus will correct the dynamics model. However, the problem persists if the dataset of environment interactions which the model is trained on is fixed.


In this work, we propose to train a policy while explicitly penalizing the mismatch between the distribution of trajectories it induces and the one reflected in the training data.
We do this by using a learned dynamics model which is unrolled for multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory.
This differentiable cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on.
We propose to measure this second cost by using the uncertainty of the dynamics model about its own predictions, calculated using dropout.
We evaluate our approach in the context of learning policies to drive an autonomous car in dense traffic, using a large-scale dataset of real-world driving trajectories which we also adapt into an environment for testing learned policies; both dataset and environment will be made public.
We show in our experiments that model-based control using this additional uncertainty regularizer substantially outperforms unregularized control, and enables learning good driving policies using \emph{only} observational data with no environment interaction or additional labeling by an expert.
Finally, we show how to effectively leverage an action-conditional stochastic forward model using a modified posterior distribution, which encourages the model to maintain sensitivity to input actions. 


\section{Model-Predictive Policy Learning with Uncertainty Regularization}

We assume we are given a dataset of observational data which consists of state-action pairs $\mathcal{D} = \{(s_t, a_t)\}_t$.
We first describe our general approach, which consists of two steps: learning an action-conditional dynamics model using the collected observational data, and then using this model to train a fast, feedforward policy network which minimizes both a policy cost and an uncertainty cost.

\subsection{Action-conditional Forward Model}

The dynamics model can be deterministic or stochastic.
A deterministic model $f_\theta(s_{1:t}, a_t)$ takes as input a sequence of observed or previously predicted states $s_{1:t}$ and an action $a_t$, and produces a prediction of the next state $\hat{s}_{t+1}$. The per-sample loss which it minimizes is given by $\ell(s_{t+1}, \hat{s}_{t+1})$, where $\ell$ is a loss function appropriate for the task at hand.
A stochastic model additionally takes as input a latent variable $z_t$ which represents the information about the next state $s_{t+1}$ which is not a deterministic function of the input.
In this work, we consider recent approaches for stochastic prediction based on Variational Autoencoders \citep{VAE, Babaeizadeh2018, Denton2018}. 
During training, latent variables $z_t$ are sampled from a distribution whose parameters are output by a posterior network $q_\phi(s_{1:t}, s_{t+1})$ conditioned on the past inputs and true targets. This network is trained jointly with the rest of the model using the reparameterization trick, and a term is included in the loss to minimize the KL divergence between the posterior distribution and a fixed prior $p(z)$, which in our case is an isotropic Gaussian.

In our work, $\ell$ represents the mean-squared error loss, and the per-sample loss used for training the stochastic model is given by:

\begin{align}
  \label{model-loss}
  \mathcal{L}(\theta, \phi ; s_{1:t}, s_{t+1}, a_t) = \|s_{t+1} - f_\theta(s_{1:t}, a_t, z_t) \|_2^2 + \beta D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) \| p(z))
\end{align}

After training, different future predictions for a given sequence of frames can be generated by sampling different latent variables from the prior distribution.

Recent models for stochastic video prediction \citep{Babaeizadeh2018, Denton2018} do not use their model for planning or training a policy network, and parameterize the posterior distribution over latent variables using a diagonal Gaussian whose mean and variance are output by $q_\phi$. 
In our case, we are training an action-conditional video prediction model which we will later use to train a policy. This leads to an additional requirement: it is important for the prediction model to accurately respond to input actions, and not use the latent variables to encode factors of variation in the outputs which are due to the actions.
To this end we propose to use a mixture of two Gaussians, with one component fixed to the prior, as our posterior distribution:


\begin{alignat}{2}
  \label{eq:update-eqn}
  &(\mu_\phi, \sigma_\phi) &&= q_\phi(s_{1:t}, s_{t+1}) \\
  &u &&\sim \mathcal{B}(p_u) \\
  &z_t &&\sim (1-u) \cdot \mathcal{N}(\mu_\phi, \sigma_\phi) + u \cdot p(z)
\end{alignat}

This is related to the $\textit{dropout approximating distribution}$ described in \citep{Gal16}, and can be seen as applying a form of global dropout to the latent variables at training time: with probability $p_u$, the latent variable vector is sampled from the fixed prior, rather than the learned posterior.
This forces the prediction model to extract as much information as possible from the input states and actions by making the latent variable independent of the output a fraction of the time. In our experiments we will refer to this parameterization as $z$-dropout. 


\subsection{Training a Policy Network with Uncertainty Minimization}
\label{uncertainty-minimization}

Once the forward model is trained, we use it to train a parameterized policy network $\pi_\psi$, which we assume to be stochastic.
We first sample an initial state sequence $s_{1:t}$ from the training set, unroll the forward model over $T$ time steps, and backpropagate gradients of a differentiable objective function with respect to the parameters of the policy network.
During this process the weights of the forward model are fixed, and only the weights of the policy network are optimized.
This objective function contains two terms: a \textit{policy cost} $C$, which reflects the underlying objective the policy is trying to learn, and an \textit{uncertainty cost} $U$, which reflects how close the predicted state induced by the policy network is to the manifold which the data $\mathcal{D}$ is drawn from.

Training the policy using a stochastic forward model involves solving the following problem, where latent variables are sampled from the prior and input into the forward model at every time step:

        \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} C(\hat{s}_{t+i}) + \lambda U(\hat{s}_{t+i}) \Big],  \mbox{ such that: }
    \begin{cases}
      z_{t+i} \sim p(z) \\
      \hat{a}_{t+i} \sim \pi_\psi(\hat{s}_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}, z_{t+i}) \\
      \end{cases}
        \end{align*}

\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=0.7\textwidth]{figures/driving/svg-crop.pdf}} \\
    \caption{Training the policy network using the stochastic forward model. Gradients with respect to costs associated with predicted states are passed through the unrolled forward model into a policy network.}
    \label{svg}
\end{figure*}
        

        This is shown in Figure \ref{svg}. The uncertainty cost $U$ is applied to states predicted by the forward model, and could reflect any measure of their likelihood under the distribution the training data is drawn from.
        We propose here a general form based on the uncertainty of the dynamics model, which is calculated using dropout.
        Intuitively, if the dynamics model is given a state-action pair from the same distribution as $\mathcal{D}$ (which it was trained on), it will have low uncertainty about its prediction.
        If it is given a state-action pair which is outside this distribution, it will have high uncertainty.



    Dropout \citep{Dropout2012, Dropout2014} is a regularization technique which consists of randomly setting hidden units in a neural network to zero with some probability.
    The work of \citep{Gal16} showed that a neural network trained with dropout is equivalent to an approximation of a probabilistic deep Gaussian Process model.
    A key result of this is that estimates of the neural network's model uncertainty for a given input can be obtained by calculating the covariance of its outputs taken over multiple dropout masks.
    We note that this uncertainty estimate is the composition of differentiable functions: each of the models induced by applying a different dropout mask is differentiable, as is the covariance operator.
    Furthermore, we can summarize the covariance matrix by taking its trace (which is equal to the sum of its eigenvalues, or equivalently the sum of the variances of the outputs across each dimension), which is also a differentiable operation. This provides a scalar estimate of uncertainty which is differentiable with respect to the input.

    More precisely, let $f_{\theta_1}, ..., f_{\theta_K}$ denote our prediction model with $K$ different dropout masks applied to its hidden units (this can also be viewed as changing its weights). We define our scalar measure of uncertainty $\Omega$ as follows:
%    Then for some set of inputs $(s_{1:t}, a_t, z_t)$, the uncertainty of the forward model is given by the covariance matrix of the outputs under the different dropout masks (up to some constants).

%    \begin{align}
%      \mbox{Cov} [\{ f_{\theta_k}(s_{1:t}, a_t, z_t) \}_{k=1}^K]
%    \end{align}


    \begin{align*}
      \Omega(s_{1:t}, a_t, z_t) &= \mbox{tr} \Big[ \mbox{Cov} [\{ f_{\theta_k}(s_{1:t}, a_t, z_t) \}_{k=1}^K] \Big] \\
      &= \sum_{j=1}^d \mbox{Var}(\{ f_{\theta_k}(s_{1:t}, a_t, z_t)_j \}_{k=1}^K)
    \end{align*}


    %This allows us to obtain gradients of the inputs, and specifically actions, with respect to this measure of model uncertainty, which gives us directions in action space which would make the model more certain.

\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=0.8\textwidth]{figures/driving/ep_uncertainty_cost-crop.pdf}} \\
    \label{planning-methods}
    \caption{Training the policy network using the differentiable uncertainty cost, calculated using dropout.}
\end{figure*}




where $d$ is the dimensionality of the output. Minimizing this quantity with respect to actions encourages the policy network to produce actions which, when plugged into the forward model, will produce predictions which the forward model is confident about.
    To compensate for differences in baseline uncertainty across different rollout lengths, we estimate the empirical mean and variance of $\Omega$ for every rollout length $t$ of the forward model over the training set, to obtain $\mu_{\Omega}^t$ and $\sigma_{\Omega}^t$. We then define our uncertainty cost as follows:

    \begin{align}
      U(\hat{s}_{t+1}) = U(s_{1:t}, a_t, z_t) = \Big [ \frac{\Omega(s_{1:t}, a_t, z_t) - \mu_\Omega^t}{\sigma_\Omega^t} \Big]_+
    \end{align}

    If the uncertainty estimate is lower than the mean uncertainty estimate on the training set for this rollout length, this loss will be zero.
    These are cases where the model prediction is within normal uncertainty ranges. If the uncertainty estimate is higher, this loss exerts a pull to change the action so that the future state will be predicted with higher confidence by the forward model.



A simple way to define $U$ given an initial sequence of states $s_{1:t}$ from $\mathcal{D}$ would be to set $U(\hat{s}_{t+k}) = \ell(\hat{s}_{t+k}, s_{t+k})$.
This would encourage the policy network to output actions which lead to a similar trajectory as the one observed in the dataset.
Although this leads to a set of states which the model is presumably confident about, it may not be a trajectory which also satisfies the policy cost $\textit{C}$.
The advantage of using the more general cost above is that, assuming the dynamics model generalizes well within its training distribution, there will be multiple trajectories which it will be confident about, and the policy network can then choose one which best satisfies the cost it is trying to optimize.
However, if the dataset $\mathcal{D}$ consists of \textit{expert} trajectories, a natural choice would be to set $C(\hat{s}_{t+i}) = U(\hat{s}_{t+i}) = \frac{1}{2} \ell(\hat{s}_{t+k}, s_{t+k})$. This give a model-based imitation learning objective which simultaneously optimizes the policy cost and the uncertainty cost.


        \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} \ell(\hat{s}_{t+i}, s_{t+i}) \Big],  \mbox{ such that: }
    \begin{cases}
      z_{t+i} \sim q_\phi(z | s_{1:t}, s_{t+1}) \\
      \hat{a}_{t+i} \sim \pi_\psi(\hat{s}_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}, z_{t+i}) \\
      \end{cases}
        \end{align*}

        We call the first approach \modelnamedrop, for Model-Predictive Policy with Uncertainty Regularization, and the second \modelnameil, for Model-Predictive Policy with Expert Regularization.
        A key feature of both approaches is that we optimize the objective over $T$ time steps, which is made possible by our learned dynamics model.
        This means that the actions will receive gradients from multiple time steps ahead, which will penalize actions which lead to large divergences from the training manifold further into the future, even if they only cause a small divergence at the next time step.

\subsection{Relationship to Bayesian Neural Networks}

Our approach can be viewed as training a Bayesian neural network (BNN) \citep{Neal1995} with latent variables using variational inference \citep{Jordan1999, VAE}. 
The distribution over model predictions for $s_{t+1}$ is given by:

\begin{equation*}
  p(s_{t+1} | s_{1:t}, a, \mathcal{D}) = \int p(s_{t+1} | f_\theta(s_{1:t}, a, z)) p(\theta, z | \mathcal{D}) d\theta dz
\end{equation*}

The distribution $p(\theta, z | \mathcal{D})$ reflects the posterior over model weights and latent variables given the data, and is intractable to evaluate.
We instead approximate it with the variational distribution $q$ parameterized by $\eta$:

\begin{align*}
  q_\eta(z, \theta) = q_\phi(z | s_{1:t}, s_{t+1}) \cdot q_{\theta^*}(\theta)
\end{align*}

Here $q_\phi$ represents a distribution over latent variables parameterized by $\phi$, which could be a diagonal Gaussian or the mixture distribution described in Section 2.1. 
%The first term is a diagonal Gaussian whose mean and variance are given by a posterior network $q_\phi(s_{1:t}, s_{t+1}))$ with parameters $\phi$.
%The second is the \textit{dropout approximating distribution} described in \citep{Gal16}.
The distribution $q_{\theta^*}$ is the \textit{dropout approximating distribution} over model parameters described in \citep{Gal16} (Section 3.2 of Supplement). 
This defines a mixture of two Gaussians with small variances over each row of each weight matrix in the forward model, with the mean of one Gaussian fixed at zero. The parameters of this distribution are the model weights, and samples can be drawn by applying different dropout masks. The parameters of the variational distribution are thus $\eta = \{ \theta^*, \phi \}$, 
and can be optimized by minimizing the variational lower bound, which is equivalent to minimizing the Kullback-Leibler divergence between the approximate posterior and true posterior:

%We instead approximate it by first defining a family of distributions $q_\eta(\theta, z)$ (parameterized by $\eta$) to approximate the true posterior,
%and then learning the parameters $\eta$ by minimizing the Kullback-Leibler divergence between the approximate and true posteriors.
%We define the approximate posterior by:


\begin{align}
  \mathcal{L}_{VLB}(\theta^*, \phi; s_{1:t}, s_{t+1}, a) &= -\int \log p(s_{t+1} | f_\theta(s_{1:t}, a, z)) q_\eta(\theta, z) d\theta dz +  D_{KL}(q_\eta(\theta, z) || p_0(\theta, z))
  \label{elbo}
\end{align}

Here $p_0(z, \theta) = p_0(z) \cdot p_0(\theta)$ represents a prior over latent variables and model parameters. By applying the chain rule for KL divergences together with the fact that $z$ and $\theta$ are independent, we obtain:

\begin{align*}
D_{KL}(q_\eta(\theta, z) || p_0(\theta, z)) &= D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) \cdot q_{\theta^*}(\theta) || p_0(z) \cdot p_0(\theta)) \\
&= D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) || p_0(z)) + D_{KL}(q_{\theta^*}(\theta | z) || p_0(\theta | z))  \\
&= D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) || p_0(z)) + D_{KL}(q_{\theta^*}(\theta) || p_0(\theta))  \\
\end{align*}
%\begin{align}
%  \mathcal{L}(\eta; s_{1:t}, s_{t+1}, a) = \int p(s_{t+1} | f_\theta(s_{1:t}, a, z)) q_\eta(\theta, z) d\theta dz + D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) || p_0(z)) + 
%  D_{KL}(q_{\theta^*}(\theta) || p_0(\theta))
%\end{align}

Setting both Gaussians in $p_0(\theta)$ to have zero mean, the second KL term becomes equivalent to scaled $\ell_2$ regularization on the model parameters, and can be set arbitrarily small (Section 4.2 in Supplement of \citep{Gal16}). 
Ignoring this term and approximating the integral in Equation \ref{elbo} using a single sample, we obtain:

\begin{align*}
  \mathcal{L}_{VLB}(\theta^*, \phi; s_{1:t}, s_{t+1}, a) &\approx -\log p(s_{t+1} | f_{\bar{\theta}}(s_{1:t}, a, z)) + D_{KL}(q_\phi(z | s_{1:t}, s_{t+1}) || p_0(z)) 
\end{align*}

where $\bar{\theta} \sim q_{\theta^*}(\theta)$ and $z \sim q_\phi(s_{1:t}, s_{t+1}))$. Assuming a diagonal Gaussian likelihood on the outputs with constant variance $1/\beta$ and scaling by $\beta$, we recover our loss term in Equation \ref{model-loss}: a variational autoencoder trained with dropout. 

Once this model is trained, for a given input we obtain an approximate distribution over outputs $p(\hat{s}_{t+1} | s_{1:t}, a)$ induced by the approximate posterior.
We now show that the covariance of the outputs $\hat{s}_{t+1}$ can be decomposed into a sum of two covariance matrices which represent the aleatoric and epistemic uncertainty, using a similar approach as \citep{depeweg18}. 
Using the conditional covariance formula we can write:

\begin{align}
  \mbox{cov}(\hat{s}_{t+1} | s_{1:t}, a) &= \mbox{cov}_\theta(\mathbb{E}_z[\hat{s}_{t+1} | s_{1:t}, a, \theta]) + \mathbb{E}_\theta[\mbox{cov}_z(\hat{s}_{t+1} | s_{1:t}, a, \theta)]
\end{align}

The first term is the covariance of the random vector $\mathbb{E}_z[\hat{s}_{t+1} | s_{1:t}, a, \theta]$ when $\theta \sim q_{\theta^*}(\theta)$.
This term ignores any contribution to the variance from $z$ and only considers the effect of $\theta$. As such it represents the epistemic uncertainty.
The second term represents the covariance of the predictions obtained by sampling different latent variables $z \sim p(z)$ averaged over different dropout masks, and ignores any contribution to the variance from $\theta$. As such it represents the aleatoric uncertainty.
Our uncertainty penalty explicitly penalizes the trace of the first matrix where the expectation over $z$ is approximated by a single sample from the prior.
Note also that the covariance matrix corresponding to the aleatoric uncertainty will change depending on the inputs. This allows our approach to handle heteroscedastic environments, where the aleatoric uncertainty will vary for different inputs.

%We mention here how our work relates to the work of \citep{depeweg18}, which also proposed to using an uncertainty penalty 

%Using the law of total expectation and conditioning on $z$, we can rewrite the first term as:

%\begin{align*}
%  \mbox{cov}_\theta(\mathbb{E}[s_{t+1} | s_{1:t}, a, \theta]) &= \mbox{cov}_\theta(\mathbb{E}_z[ \mathbb{E}[[s_{t+1} | s_{1:t}, a, \theta, z]]]) \\
%  &= \mbox{cov}_\theta(\mathbb{E}_z[s_{t+1}])
%\end{align*}

%Applying the conditional covariance formula to the second term, this time conditioning on $z$, we can write:

%\begin{align}
%\mathbb{E}_\theta[\mbox{cov}(s_{t+1} | s_{1:t}, a, \theta)] &= 
%\end{align}


%We define the variational distribution $q_\eta(\theta, z)$ to be the product of two distributions:
%$q_\eta(\theta, z) = q_\phi(z | s_{1:t}, s_{t+1}) q_{\theta^*}(\theta)$. 
%a distribution over latent variables $p(z | q_\phi(s_{1:t}, s_{t+1}))$, and a distribution over model weights $p_{\theta^*}(\theta)$.
%The first is a diagonal Gaussian whose mean and variance are output by the posterior network $q_\phi(s_{1:t}, s_{t+1}))$. 
%The second is the \textit{dropout approximating distribution} described in \citep{Gal16}.
%Namely, the distribution over each weight matrix in the model is a mixture of two Gaussians with small variances, with the mean of one Gaussian fixed at zero. Samples from this distribution can be drawn by applying different dropout masks to the model weights, and the parameters of the distribution are simply the model weights learned by gradient descent with dropout applied at training time. The parameters of the variational distribution are thus $\eta = \{ \theta^*, \phi \}$.







\section{Dataset and Planning Environment}
\label{dataset-and-planning}

We apply our approach to learn driving policies using a large-scale dataset of driving videos taken from traffic cameras.
%To begin with, we summarize the dataset and planning environment which we used, with full details provided in Appendix \ref{i80-dataset-prep}.
The Next Generation Simulation program's Interstate 80 (NGSIM I-80) dataset \citep{NGSIM} consists of 45 minutes of recordings from traffic cameras mounted over a stretch of highway.
The driver behavior is complex and includes sudden accelerations, lane changes and merges which are difficult to predict; as such the dataset has high enviroment stochasticity.
After recording, a viewpoint transformation is applied to rectify the perspective, and vehicles are identified and tracked throughout the video.
This yields a total 5596 car trajectories, which we split into training ($80\%$), validation ($10\%$) and testing sets ($10\%$). In all, the dataset contains approximately 2 million transitions. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/driving/I-80-blue-crop.pdf}
  \caption{
    Preprocessing pipeline for the NGSIM-I80 data set.
    Orange arrows show same vehicles across stages.
    Blue arrows show corresponding extracted context state.
    (a) Snapshots from two of the seven cameras.
    (b) View point transformation, car localisation and tracking.
    (c) Context states are extracted from rectangular regions surrounding each vehicle.
    (d) Five examples of context states $i_t$ extracted at the previous stage.
  }
\label{I-80}
\end{figure}


We then applied additional preprocessing to obtain a state and action representation $(s_t, a_t)$ for each car at each time step, suitable for learning an action-conditional predictive model.
Our state representation $s_t$ consists of two components: an image $i_t$ representing the neighborhood of the car, and a vector $u_t$ representing its current position and velocity.
The images $i_t$ are centered around the ego car and encode both the lane emplacements and the locations of other cars.
Each image has 3 channels: the first (red) encodes the lane markings, the second (green) encodes the locations of neighboring cars, which are represented as rectangles reflecting the dimensions of each car, and the third channel (blue) represents the ego car, also scaled to the correct dimensions.
This is summarized in Figure \ref{I-80}. The action $a_t$ at a given time step consists of a 2-dimensional vector representing the acceleration/braking of the car and its change in steering angle.
We also define two cost functions which together make up the policy cost: a proximity cost which reflects how close the ego car is to neighboring cars, and a lane cost which reflects how much the ego car overlaps with lane markings. These are represented as a cost vector at each timestep, $c_t = (C_{\text{proximity}}(s_t), C_{\text{lane}}(s_t))$. Full details can be found in Appendix \ref{i80-dataset-prep}.


We also adapted this dataset to be used as an environment to evaluate learned policies, with the same interface as OpenAI Gym \citep{OpenAIBaselines}.
Choosing a policy for neighboring cars is challenging due to a cold-start problem: to accurately evaluate a learned policy, the other cars would need to follow human-like policies which would realistically react to the controlled car, which are not available.
We take the approach of letting all the other cars in the environment follow their trajectories from the dataset, while a single car is controlled by the policy we seek to evaluate.
This approach avoids hand-designing a policy for the neighboring cars which would likely not reflect the diverse nature of human driving.
The limitation is that the neighboring cars do not react to the controlled car, which likely makes the problem more difficult as they do not try to avoid collisions.


\section{Related Work}

A number of authors have explored the use of learned, action-conditional forward models which are then used for planning, starting with classic works in the 90's \citep{Nguyen1990, Schmidhuber1990, Jordan1992}, and more recently in the context of video games \citep{Oh15, Pascanu17, I2A}, robotics and continous control \citep{FinnGL16, Poke, Nagabandi2017, UPN}.
Our approach to learning policies by backpropagating through a learned forward model is related to the early work of \citep{Nguyen1989} in the deterministic case, and the SVG framework of \citep{SVG} in the stochastic case. However, neither of these approaches incorporates a term penalizing the uncertainty of the forward model when training the policy network.

The works of \citep{DeepPilco, Chua2018} also used model uncertainty estimates calculated using dropout in the context of model-based reinforcement learning, but used them for sampling trajectories during the forward prediction step. Namely, they applied different dropout masks to simulate different state trajectories which reflect the distribution over plausible models, which were then averaged to produce a cost estimate used to select an action.

Our model uncertainty penalty is related to the cost used in \citep{Kahn2017}, who used dropout and model ensembling to compute uncertainty estimates for a binary action-conditional collision detector for a flying drone. These estimates were then used to select actions out of a predefined set which yielded a good tradeoff between speed, predicted chance of collision and uncertainty about the prediction. In our work, we apply uncertainty estimates to the predicted high-dimensional states of a forward model at every time step, summarize them into a scalar, and backpropagate gradients through the unrolled forward model to then train a policy network by gradient descent.

The work of \citep{depeweg18} also proposed adding an uncertainty penalty when training paramaterized policies, but did so in the context of BNNs trained using $\alpha$-divergences applied in low-dimensional settings, whereas we use variational autoencoders combined with dropout for high-dimensional video prediction. 
$\alpha$-BNNs can yield better uncertainty estimates than variational inference-based methods, which can underestimate model uncertainty by fitting to a local mode of the exact posterior \citep{depeweg16, alpha-bnn}. However, they also require computing multiple samples from the distribution over model weights when training the forward model, which increases memory requirements and limits scalability to high-dimensional settings such as the ones we consider here. 

%$\alpha$-BNNs place a factorized Gaussian prior both over latent variables and network weights, and learn the parameters of these distributions by minimizing an energy function whose minimizer corresponds to a local minimum of alpha-divergences.
%Our approach places a Gaussian or masked Gaussian prior over latent variables and a mixture of Gaussian prior over network weights, and optimizes the parameters of these distributions using variational inference. 
%Both methods have different advantages. An advantage of $\alpha$-BNNs over Dropout VI (pointed out in [1, 2]) is that Dropout VI can underestimate model uncertainty by fitting to a local mode of the exact posterior, whereas $\alpha$-divergence minimization can give better coverage of the distribution. A disadvantage of $\alpha$-BNNs (pointed out by [2]) is that they require significant changes in existing deep learning models and code bases, and the functions they optimize are less intuitively interpretable by non-experts. 


%Our approach uses a different class of methods which are more easily scalable to the high-dimensional settings we consider.


The problem of covariate shift when executing a policy learned from observational data has been well-recognized in imitation learning.
It was first noted in the early work of \citep{Pomerleau91}, and was shown in \citep{Ross2010EfficientRF} to cause a regret bound which grows quadratically in the time horizon of the task.
The work of \citep{Dagger} proposed a method to efficiently use expert feedback if available, which has also been applied in the context of autonomous driving \citep{Zhang16}.
Our approach also addresses covariate shift, but does so without querying an expert.

Our \modelnameil approach is related to the work of \citep{Englert2013}, who also performed imitation learning at the level of trajectories rather than individual actions. They did so in low-dimensional settings using Gaussian Processes, whereas our method uses an unrolled neural network representing the environment dynamics which can be applied to high-dimensional state representations. The work of \citep{Baram2017EndtoEndDA} also used a neural network dynamics model in the context of imitation learning, but did so in the interactive setting to minimize a loss produced by a discriminator network.


Several works have applied deep learning methods in the context of autonomous driving. The works of \citep{Pomerleau91, LeCun2006, Bojarski16, Pan17} used neural networks to control policies trained by imitation learning, while \citep{Williams2017} learned models of the vehicle dynamics. These works focused on lane following or avoiding static obstacles in visually rich environments and did not consider settings with dense moving traffic, which we focus on in this work. The work of \citep{Sadigh16} developed a model of the interactions between the two drivers which was then used to plan actions in simple settings, using symbolic state representations. In our work, we consider the problem of learning driving policies in dense traffic, using high-dimensional state representations which reflect the neighborhood of the ego car. %The work of \citep{Zhang16} also considered a setting with other cars, and used imitation learning together with an efficient method of querying the actions of an expert to


%Our model-based imitation learning approach is related to the work of \citep{Englert2013}, who also performed imitation learning at the level of trajectories rather than actions.
%They worked with low-dimensional state vectors representing real robots arms, which allowed them to use Gaussian Processes, and the primary source of uncertainty was measurement error. Gaussian Processes are highly data efficient, but do not scale well to high dimensions. In our setting, states include high-dimensional images and we are trying to model uncertain behavior of other drivers. The work of \citep{baram17} also used an unrolled forward model for imitation learning, but did so in the context of Generative Adversarial Imitation Learning and in deterministic environments.

%Our approach to learning policies which minimize predicted costs fits within the conceptual framework of Stochastic Value Gradients (SVG) \citep{SVG}, and extends it to a setting with high-dimensional state representations.
%This requires us to use more sophisticated stochastic models than the ones in the original work, which used additive Gaussian noise whose parameters were learned using the reparamaterization trick.
%They also considered an online setting where the agent continues to collect experience, which reduces the need for the epistemic uncertainty penalty, whereas we found this to be essential in our setting where we learn purely from observational data.




\section{Experiments}
\label{experiments}

We now report experimental results. We designed a deterministic and stochastic forward model to model the state and action representations described in Section \ref{dataset-and-planning}, using convolutional layers to process the images $i_t$ and fully-connected layers to process the vectors $u_t$ and actions $a_t$. All model details can be found in Appendix \ref{model-details} and training details can be found in Appendix \ref{training-details-appendix}.
Additional video results for the model predictions and learned policies can be found at the following URL: \url{https://sites.google.com/view/model-predictive-driving/home}.


    \subsection{Prediction Results}


    We first generated predictions using both deterministic and stochastic forward models, shown in  Figure \ref{prediction-results}.
    The deterministic model produces predictions which become increasingly blurry further into the future. Our stochastic model produces predictions which stay sharp far into the future.
    By sampling different sequences of latent variables, different future scenarios are generated.
    Note that the two sequences generated by the stochastic model are different from the ground truth future which occurs in the dataset.
    This is normal as the future observed in the dataset is only one of many possible ones.
    Additional video generations can be viewed at the URL.



\begin{figure*}[t!]
    \centering
    \subfigure[Ground truth sequence]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/truth-crop.pdf}}
    \subfigure[Deterministic Model]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/det-crop.pdf}} \\
    \subfigure[Stochastic Model, sample 1]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/pred1-crop.pdf}}
    \subfigure[Stochastic Model, sample 2]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/pred2-crop.pdf}} \\
    \caption{Video prediction results using a deterministic and stochastic model over 200 time steps (images are subsampled across time). Two different future predictions are generated by the stochastic model by sampling two different sequences of latent variables. The deterministic model averages over possible futures, producing blurred predictions.}
    \label{prediction-results}
\end{figure*}









    \subsection{Policy Evaluation Results}

    We evaluated several different policies using two measures: whether the controlled car reaches the end of the road segment without colliding into another car or driving off the road, and the distance travelled before the episode ends. Policies which collide quickly will travel shorter distances. All cars are initialized at the beginning of the road segment with the initial speed they were driving at in the dataset, and then are controlled by the policy being measured. We only report performance for cars in the testing trajectories, which were not used when training the forward model or policy network. 



  \begin{table}[t]
    \centering
  \begin{tabular}{|ll|ll|}
    \hline
    Method & Model & Mean Distance & Success Rate (\%)  \\
    \hhline{|====|}
    Human & & $209.4$ & $100.0$ \\
    \hline
    No action & & $87.3$ & $16.2$ \\
    \hline
    1-step IL & & $73.8 \pm 7.9$ & $7.3 \pm 4.1$ \\
    \hline
    SVG & stochastic & $17.1 \pm 4.3$ & $0.0 \pm 0.0$ \\
    VG & deterministic & $11.9 \pm 4.2$ & $0.0 \pm 0.0$ \\
    \hline
    MPUR & stochastic+$z$-dropout & $171.2 \pm 4.5$ & $74.8 \pm 3.0$ \\
    MPUR & stochastic & $166.8 \pm 2.4$ & $71.8 \pm 1.0$ \\
    MPUR & deterministic & $162.4 \pm 2.8$ & $69.1 \pm 1.6$ \\
    MPER & stochastic & $70.0 \pm 8.0$ & $4.6 \pm 2.1$ \\
    MPER & deterministic & $157.4 \pm 0.7$ & $63.7 \pm 0.5$ \\
    \hline
  \end{tabular}
  \label{main-table}
  \caption{Best results across different rollouts lengths.} 
  \end{table}
    

    We compared our approach against several baselines which can also learn from observational data, which are described in Figure \ref{performance}.
    All policy networks have the same architecture, and all are fed the concatenation of the 20 previous states as input. They all output the parameters of a 2D diagonal Gaussian over action space, from which the next action is sampled.
    The policy cost which we minimize for VG, SVG and \modelnamedrop is given by:

    \begin{equation}
      C = C_{\text{proximity}} + 0.2 \cdot C_\text{lane}
    \end{equation}

    where $C_{\text{proximity}}$ and $C_\text{lane}$ are the proximity and lane costs described in Section \ref{dataset-and-planning}. This puts a higher priority on avoiding other cars while still encouraging the policy to stay within the lanes. \modelnamedrop additionally minimizes $U$, the model uncertainty cost described in Section \ref{uncertainty-minimization}.


\begin{figure*}[t!]
  \centering
    \subfigure{\includegraphics[width=0.46\textwidth]{figures/driving/rollout_length_distance-crop.pdf}}
    \subfigure{\includegraphics[width=0.46\textwidth]{figures/driving/rollout_length_success-crop.pdf}}
    \subfigure{

      \begin{tabularx}{\textwidth}{|l| X|}
    \hline
    \textbf{Human} & The human trajectories observed in the testing set, which are all collision-free. \\
    \hline
    \textbf{No action} & A policy which outputs an action of zero, maintaining constant speed and direction.\\
    \hline
    \textbf{1-step IL} & A policy network trained with single-step imitation learning to minimize the negative log-likelihood of the next human action observed in the dataset. \\
    \hline
    \textbf{SVG} & A policy network trained with stochastic value gradients, using our stochastic forward model. Here the latent variables are inferred using the posterior network for a state trajectory sampled from the training set. This is the same setup as \citep{SVG}, with the difference that the agent does not interact with the environment and learns from a fixed observational dataset. \\
    \hline
    \textbf{VG} & A policy trained with value gradients, using the deterministic forward model. This is similar to SVG, but does not involve latent variables. \\
    \hline
    \textbf{\modelnamedrop} & A policy trained with \modelnamedrop, using a deterministic or stochastic model. A cost term is included to penalize the uncertainty of the dynamics model. \\    
    \hline
    \textbf{\modelnameil} & A policy trained with \modelnameil, using a deterministic or stochastic model. The policy is trained to match expert trajectories from the training set. \\
    \hline
  \end{tabularx}
  }
    \caption{Performance of policies learned using different methods on the NGSIM dataset, measured in success rate and distance travelled. Including a cost term penalizing the dynamics model's uncertainty is essential for good performance. Using the modified posterior distribution ($z$-dropout) improves the performance of the stochastic model. Training policies by performing longer rollouts through the environment dynamics model also significantly improves performance.}
    \label{performance}
\end{figure*}





Figure \ref{performance} compares performance for the different methods. 
The 1-step imitation learner, SVG and VG all perform poorly, and do not beat the simple baseline of performing no action.
Both \modelnamedrop and \modelnameil significantly outperform the other methods. 
Videos of the learned policies for both \modelnameil and \modelnamedrop driving in the environment can be found at the URL.
The policies learn effective behaviors such as braking, accelerating and turning to avoid other cars.
Figure \ref{trajectories} shows trajectories on the map for different methods. We see that the single-step imitation learner produces divergent trajectories which turn into other lanes, whereas the \modelnamedrop and \modelnameil methods show trajectories which primarily stay within their lanes.

\modelnamedrop become equivalent to VG in the deterministic setting if we remove the uncertainty penalty, and the large difference in performance shows that including this penalty is essential.
Figure \ref{svg-pred} shows the average predicted policy cost and uncertainty cost of the two methods.
VG produces much lower predicted policy cost, yet very high uncertainty cost. This indicates that the actions the policy produces induce a distribution over states which the forward model is highly uncertain about. The policy trained with \modelnamedrop produces higher policy cost estimates, but lower uncertainty cost, and performs much better when executed in the environment.

The stochastic model trained with a standard Gaussian posterior yields limited improvement over the deterministic model. 
However, the stochastic model trained with the $z$-dropout parameterization yields a significant improvement. 
%The stochastic model trained with a standard Gaussian posterior yields a relatively minor improvement over the deterministic in the case of \modelnamedrop, but this difference is not statistically significant. 
%The stochastic model trained with a standard Gaussian posterior yields an improvement over the deterministic model in the case of stochastic \modelnamedrop, despite the improvement in terms of visual quality shown in Figure \ref{prediction-results}. %, and gives much \textit{worse} performance for \modelnameil.
At the URL we compare action-conditional predictions using both models.
The stochastic model which uses the standard posterior is less responsive to input actions than the model trained with the modified posterior. 
%We also found that using the modified $z$-dropout parameterization increases the sensitivity of the forward model to the input actions, as can be seen at the URL. 
This lack of responsiveness is especially pronounced when the model is given latent variables inferred by the posterior network, as opposed to latent variables which are sampled from the prior.
%We also include a comparison of predictions of the forward model conditioned on different action sequences, using latent variables inferred or sampled from the prior at the URL.
We also trained a policy network using \modelnamedrop with inferred rather than sampled latent variables, and observed a large drop in performance (see Appendix \ref{action-sensitivity-appendix}).

A possible explanation is that the forward model encodes some factors of variation of the output due to the actions in its latent variables.
The sequence of latent variables sampled from the prior are independent, which may cause these effects to cancel each other over the sequence. However, the latent variables inferred from a sequence of states in the training set are highly dependent, and together they may explain away the effects of the actions.
Our modified posterior distribution, which samples the latent variable from the prior rather than posterior part of the time, encourages the network to extract as much information as possible from the input actions. This is because factors of variation in the output which are due to the actions cannot be encoded in the latent variables when they are sampled from the prior. 


Figure \ref{performance} shows the performance of \modelnamedrop and \modelnameil for different rollout lengths.
All methods see their performance improve dramatically as we increase the rollout length, which encourages the distribution of states the policy induces and the training distribution to match over longer time horizons. We also see that the stochastic model $z$-dropout outperforms the standard stochastic model as well as the deterministic model over most rollout lengths. 
%\modelnameil performs similarly to the 1-step imitation learner when given a rollout length of 1.
%This makes sense as they both optimize similar objectives.
%However, \modelnameil's performance improves dramatically as we increase the rollout length, which encourages the distribution of states the policy induces and the expert's distribution to match over longer time horizons.
%The performance of \modelnamedrop also increases with rollout length. Both methods achieve similar performance for longer rollouts.







\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=\textwidth]{figures/driving/trajectories-w-crop.pdf}}
    \caption{Driving trajectories for \modelnamedrop, \modelnameil and a single-step imitation learner policies over the stretch of highway. The single-step imitation learner policy often veers across lanes, whereas the other policies stay within their lanes. These policies also change their speed to avoid collisions (not shown, see URL).}
    \label{trajectories}
\end{figure*}




\begin{figure*}[t!]
    \centering
%    \subfigure[with uncertainty cost]{\includegraphics[width=0.48\textwidth]{figures/driving/svg_reg_prediction-crop.pdf}}
%    \subfigure[without uncertainty cost]{\includegraphics[width=0.48\textwidth]{figures/driving/svg_no_reg_prediction-crop.pdf}} \\
    \subfigure{

  \begin{tabular}{|lrrrr|}
    \hline
    Method & Mean Distance (m) & Success Rate (\%) & Total Predicted Cost & $U$ \\
    \hhline{|=====|}
    VG & 14.9 & 0.0 & 0.03 & 4087.4 \\
    \modelnamedrop & 168.8 & 73.5 & 0.13 & 0.4 \\
    \hline
  \end{tabular}
    }
    \caption{Policy and uncertainty costs with and without uncertainty regularization. The policy trained with unregularized VG exploits errors in the forward model to produce actions which yield low predicted cost but high uncertainty. Including the uncertainty cost yields higher predicted cost, but better performance when the policy is executed in the environment.}
    \label{svg-pred}
\end{figure*}


%We ran an additional experiment where we trained a forward model which samples the latent variables from the prior, rather than the posterior, with probability $0.5$, in an effort to encourage it to extract more information from the actions.

%  We additionally performed several ablation experiments to understand the effects of different modeling choices, shown in table \ref{ablation}.
%  Using the action buffer has a non-negligible effect on the SMPC planning, as we see a performance drop when starting from newly initialized actions every time planning is required rather than initializing with a previously optimized action sequence.
%  It is possible that with larger numbers of gradient steps this difference in performance would decrease, but the current setup is already very expensive.
%  We also compared our TEN model to a VAE model with the same architecture and hyperparameters, except for the $\beta$ hyperparameter representing the weight of the KL term in the loss, which we set to $10^{-6}$ (we optimized over the range $\{ 1, 10^{-1},..., 10^{-6} \}$ and found that values higher than $10^{-5}$ produced blurry predictions similar to the deterministic model.
%  Performance is very similar for both models.

%  Finally, we compare SVG performance with and without masking the $z$ variables during training.
%  Not including the masking causes a performance drop due to the forward model becoming less sensitive to the actions.


  \section{Conclusion}

  In this work, we have proposed a general approach for learning policies from purely observational data. A key element is the use of a learned dynamics model which is used to optimize a policy cost over multiple time steps, and an uncertainty term which penalizes the divergence of the trajectories induced by the policy from the manifold it was trained on.
  We have applied this approach to a large observational dataset of real-world traffic recordings, and shown it can effectively learn policies for navigating in dense traffic, which outperform other approaches which learn from observational data.
  However, there is still a sizeable gap between the performance of our learned policies and human performance.
  We release both our dataset and environment, and encourage further research in this area to help narrow this gap.
  We also believe this provides a useful setting for evaluating generative models in terms of their ability to produce good policies.
%  The dataset exhibits high uncertainty, and our stochastic model performs much better than the deterministic model in terms of visual generations.
%  However, our current setup does not exploit this, and
%  In particular, the dataset contains human driving trajectories with high uncertainty, which suggests that stocahs
  An interesting next step would be to optimize actions or policies to produce more complex and targeted behaviors, such as changing to a specified lane while avoiding other cars.
%  In the current setup, the policies are optimized only to avoid collisions and stay within lanes when possible, whereas in a real-world scenario we would want a policy which can safely navigate among traffic to different locations.
  Finally, our approach is general and could potentially be applied to many other settings where interactions with the environment are expensive or unfeasible, but observational data is plentiful.




\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\appendix

\section{Dataset and Planning Environment}
\label{i80-dataset-prep}

To begin with, we describe the details and preparation of the dataset and planning environment which we used, which are summarized in Figure \ref{I-80}.
The Next Generation Simulation program's Interstate 80 (NGSIM I-80) dataset \citep{NGSIM} consists of 45 minutes of recordings made of a stretch of highway in the San Francisco Bay Area by cameras mounted on a 30-story building overlooking the highway. The recorded area includes six freeway lanes (including a high-occupancy vehicle lane) and an onramp.
The driver behavior is complex and includes sudden accelerations, lane changes and merges which are difficult to predict; as such the dataset has high aleatoric uncertainty.
There are three time segments, each of 15 minutes, taken at different times of day which capture the transition between uncongested and congested peak period conditions.
After recording, a viewpoint transformation is applied to rectify the perspective, and vehicles are identified and tracked throughout the video; additionally, their size is inferred.
This yields a total 5596 car trajectories, represented as sequences of coordinates $\{x_t, y_t\}$. We split these trajectories into training ($80\%$), validation ($10\%$) and testing sets ($10\%$).

We then applied additional preprocessing to obtain suitable representations for learning a predictive model.
Specifically, we extracted the following: i) a state representation for each car at each time step $s_t$, which encodes the necessary information to choose an action to take, ii) an action $a_t$ which represents the action of the driver, and iii) a cost $c_t$, which associates a quality measure to each state. We describe each of these below.


\textbf{State representation}:
Our state representation consists of two components: an image representing the neighborhood of the car, and a vector representing its current position and velocity.
For the images, we rendered images centered around each car which encoded both the lane emplacements and the locations of other cars.
Each image has 3 channels: the first (red) encodes the lane markings, the second (green) encodes the locations of neighboring cars, which are represented as rectangles reflecting the dimensions of each car, and the third channel (blue) represents the ego car, also scaled to the correct dimensions.
All images have dimensions $3 \times 117 \times 24$, and are denoted by $i_t$.
\footnote{Another possibility would have been to construct feature vectors directly containing the exact coordinates of neighboring cars, however this presents several difficulties.
First, cars can enter and exit the neighborhood, and so the feature vector representing the neighboring cars would either have to be dynamically resized or padded with placeholder values.
Second, this representation would not be permutation-invariant, and it is unclear where to place a new car entering the frame.
Third, encoding the lane information in vector form would require a parametric representation of the lanes, which is more complicated.
Using images representations naturally avoids all of these difficulties.}
Two examples are shown in Figure \ref{cost}.
We also computed vectors $u_t = (p_t, \Delta p_t)$, where $p_t = (x_t, y_t)$ is the position at time $t$ and $\Delta p_t = (x_{t+1} - x_t, y_{t+1} - y_t)$ is the velocity.


\begin{figure}
  \centering
  \subfigure[19.8 km/h]{
  \includegraphics[height=0.3\textheight]{figures/driving/image_198-crop.pdf}
  \includegraphics[height=0.3\textheight]{figures/driving/mask_198-crop.pdf}
  }
  \hspace{15mm}
  \subfigure[50.3 km/h]{
  \includegraphics[height=0.3\textheight]{figures/driving/image_503-crop.pdf}
  \includegraphics[height=0.3\textheight]{figures/driving/mask_503-crop.pdf}
  }
  \caption{Image state representations and proximity cost masks for cars going at different speeds. The higher the speed, the longer the safety distance required to maintain low cost.}
\label{cost}
\end{figure}







%Every vehicle in the simulator is initialised with starting position $\bm{p} = (x, y)$, initial velocity $v \bm{d}$ (where $v$ represents the speed and $\bm{d}$ the direction versor), correct dimensions, and identification (ID) number.
%At every time step $\Delta t = 0.1\,\text{s}$, we use the tracked trajectories (previously smoothed with a $1.5\,\text{s}$ running average) and a simplified kinematic model of a car to compute the agent actions $(a, b)$, corresponding to acceleration and tangential deviation.
%Therefore, the vehicle internal state is updated with: $\bm{p} \gets \bm{p} + v \bm{d} \Delta t$, $v \gets v + a \Delta t$, $\bm{d} \gets \bm{d} + v b \bm{d}_\perp \Delta t$, and $\bm{d} \gets \bm{d} / \lVert \bm{d} \rVert$.
%A longitudinal and transverse inter-vehicle linear proximity cost is computed, which is maximum in case of collision and goes to zero if vehicles are sufficiently spaced.
%Finally, the vehicles are drawn onto the screen canvas and displayed with their corresponding ID number.
%\textbf{(d)} Lanes and vehicles are also drawn on a secondary canvas, from which the context states are extracted.
%Rectangular regions around each vehicle --- oriented according to $\bm{d}$, of length corresponding to twice the space travelled in $1\,\text{s}$ at $130\,\text{km/h}$, of width equal four times the lane width --- scaled by a $0.5$ factor constitute the context states.
%    At this stage, the lane-crossing cost is computed using a modified morphological distance transform between each vehicle and the lane channel.



\textbf{Action representation}: Each action vector $a_t$ consists of two components: an acceleration (which can be positive or negative) which reflects the change in speed, and a change in angle.
The acceleration at a given time step is computed by taking the difference between two consecutive speeds, while the change in angle is computed by projecting the change in speed along its orthogonal direction:

\begin{align*}
  \Delta \mbox{speed} &= \| \Delta p_{t+1} \|_2 - \| \Delta p_t \|_2 \\
  \Delta \mbox{angle} &= (\Delta p_{t+1} - \Delta p_t)^\top (\Delta p_t)_\perp / \| \Delta p_t \|_2  \\
  a_t &= (\Delta \mbox{speed}, \Delta \mbox{angle}) \\
\end{align*}




\textbf{Cost}: Our cost function has two terms: a proximity cost and a lane cost. The proximity cost reflects how close the ego car is to neighboring cars, and is computed using a mask in pixel space whose width is equal to the width of a lane and whose height depends on the speed of the car. Two examples are shown in Figure \ref{cost}.
This mask is pointwise multiplied with the green channel, and the maximum value is taken to produce a scalar cost.
The lane cost uses a similar mask fixed to the size of the car, and is similarly multiplied with the red channel, thus measuring the car's overlap with the lane.
Both of these operations are differentiable so that we can backpropagate gradients with respect to these costs through images predicted by a forward model.

This preprocessing yields a set of state-action pairs $(s_t, a_t)$ (with $s_t=(i_t, u_t)$) for each car, which constitute the dataset we used for training our prediction model.
We then use the cost function to optimize action sequences at planning time, using different methods which we describe in Section \ref{planning-methods}.

We now describe how we adapted this dataset to be used as an environment to evaluate planning methods.
Building an environment for evaluating policies for autonomous driving is not obvious as it suffers from a cold-start problem.
Precisely measuring the performance of a given driving policy would require it to be evaluated in an environment where all other cars follow policies which accurately reflect human behavior.
This involves reacting appropriately both to other cars in the environment as well as the car being controlled by the policy being evaluated.
However, constructing such an environment is not possible as it would require us to already have access to a policy which drives as humans do, which in some sense is our goal in the first place. One could hand-code a driving policy to control the other cars in the environment, however is it not clear how to do so in a way which accurately reflects the diverse and often unpredictable nature of human driving.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/driving/traffic_gym_code-crop.pdf}
  \caption{NGSIM planning environment.}
\label{interface}
\end{figure}


We adopt a different approach where we let all other cars in the environment follow their trajectories in the dataset, while controlling one car with the policy we seek to evaluate.
The trajectory of the controlled car is updated as a function of the actions output by the policy, while the trajectories of the other cars remain fixed.
If the controlled car collides with another car, this is recorded and the episode ends.
This approach has the advantage that all other cars in the environment maintain behavior which is close to human-like.
The one difference with true human behavior is that the other cars do not react to the car being controlled or try to avoid it, which may cause crashes which would not occur in real life.
The driving task is thus possibly made more challenging than in a true environment, which we believe is preferable to using a hand-coded policy.
The interface is set up the same way as environments in OpenAI Gym \citep{OpenAIBaselines}, and can be accessed with a few lines of Python code, as shown in Figure \ref{interface}.



\section{Model Details}
\label{model-details}

The architecture of our forward model consists of four neural networks: a state encoder $f_\text{enc}$, an action encoder $f_\text{act}$, a decoder $f_\text{dec}$, and the posterior network $f_\phi$.
At every time step, the state encoder takes as input the concatenation of 20 previous states, each of which consists of a context image $i_t$ and a 4-dimensional vector $u_t$ encoding the car's position and velocity.
The images $i_{t-20}, ..., i_t$ are run through a 3-layer convolutional network with 64-128-256 feature maps, and the vectors $u_{t-20}, ..., u_t$ are run through a 2-layer fully connected network with 256 hidden units, whose final layers contain the same number of hidden units as the number of elements in the output of the convolutional network (we will call this number $n_H$).
The posterior network takes the same input as the encoder network, as well as the the ground truth state $s_{t+1}$, and maps them to a distribution over latent variables, from which one sample $z_t$ is drawn.
This is then passed through an expansion layer which maps it to a representation of size $n_H$.
The action encoder, which is a 2-layer fully-connected network, takes as input a 2-dimensional action $a_t$ encoding the car's acceleration and change in steering angle, and also maps it to a representation of size $n_H$.
The representations of the input states, latent variable, and action, which are all now the same size, are combined via addition.
The result is then run through a deconvolutional network with 256-128-64 feature maps, which produces a prediction for the next image $i_{t+1}$, and a 2-layer fully-connected network (with 256 hidden units) which produces a prediction for the next state vector $u_{t+1}$. These are illustrated in Figure \ref{model-components}.


\begin{figure}[t!]
    \centering
    \subfigure[$f_{enc}$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_enc-crop.pdf}} \\
    \subfigure[$f_{dec}$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_dec-crop.pdf}} \\
    \subfigure[$q_\phi$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_phi-crop.pdf}} \\
    \caption{Individual components of the prediction model.}
\end{figure}
\label{model-components}



The specific updates of the stochastic forward model are given by:

\begin{alignat}{2}
  \label{eq:update-eqn}
  &(\mu_\phi, \sigma_\phi) &&= q_\phi(s_{1:t}, s_{t+1}) \\
  &\epsilon &&\sim \mathcal{N}(0, I) \\
  &z_t &&= \mu_\phi + \sigma_\phi \cdot \epsilon \\
  &\hat{s}_{t+1} = (\tilde{i}_{t+1}, \tilde{u}_{t+1}) &&= f_\theta(s_{1:t}, a_t, z_t)
%  &\ell(f_\theta(s_{1:t}, a_t, z_t), s_{t+1}) + \beta D_{\mbox{KL}}(f_\phi(s_{1:t}, s_{t+1}), \mathcal{N}(0, I))
\end{alignat}

The per-sample loss is given by:

\begin{align}
  \label{eq:update-eqn}
  \ell(s_{1:t}, s_{t+1}) = \|\tilde{i}_t - i_t \|_2^2 + \| \tilde{u}_t - u_t \|_2^2 + \beta D_{KL}(\mathcal{N}(\mu_\phi, \sigma_\phi) || p(z))
\end{align}

We also train a cost predictor which takes as input the states predicted by the forward model and produces a two-dimensional output (one output for the proximity cost, and one output for the lane cost). 
This consists of a 3-layer encoder followed by a two-layer fully connected network with sigmoid non-linearities as the end to constrain the values between $0$ and $1$. 



    \section{Training Details}
    \label{training-details-appendix}

    \subsection{Forward Model}
    We trained our prediction model in deterministic mode ($p=0$) for 200,000 updates, followed by another 200,000 updates in stochastic mode.
    We save the model after training in deterministic mode and treat it as a deterministic baseline.
    Our model was trained using Adam \citep{ADAM} with learning rate 0.0001 and minibatches of size 64, unrolled for 20 time steps, and with dropout ($p_{dropout}=0.1$) at every layer, which was necessary for computing the epistemic uncertainty cost when training the policy network.

    \subsection{Policy Models}

    All policy networks have the same architecture: a 3-layer ConvNet with feature maps of size 64-128-256, followed by 3 fully-connected layers with 256 hidden units each, with the last layer outputting the parameters of a 2D Gaussian distribution over actions. All policy networks are trained with Adam with learning rate 0.0001. The \modelnameil and \modelnamedrop policies are trained by backpropagation through the unrolled forward model using the reparamaterization trick \citep{VAE}. The single-step imitation learner is trained to directly minimize the negative log-likelihood of the ground truth action in the dataset under the parameters output by the policy network. All MPUR policies use a weighting of $\lambda=0.5$ for the uncertainty cost. Additionally, we detach gradients of predicted costs coming into the states, to prevent the policy from lowering the predicted cost (which is speed-dependent) by slowing down. We found that not doing this can result in the policy slowing excessively, and then attempting to speed up only when another car gets close. We repeat the policy training with 3 random seeds for each method. 
    



    \section{Action Sensitivity}
    \label{action-sensitivity-appendix}


\begin{figure*}[h]
    \centering
    \subfigure{

  \begin{tabular}{|lrr|}
    \hline
    Method & Mean Distance & Success Rate \\
    \hhline{|===|}
    \modelnamedrop, $z_t \sim p(z)$ & 976 & 59.1 \\
    \modelnamedrop, $z_t \sim q_\phi(s_{1:t}, s_{t+1})$ & 558 & 19.2 \\
    \hline
  \end{tabular}
    }
    \label{action-sens}
\end{figure*}






\end{document}
