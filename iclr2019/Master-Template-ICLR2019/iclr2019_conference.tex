\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}
\usepackage{graphicx} % more modern
\usepackage{subfigure}
\usepackage{hhline}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{wrapfig}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


%\title{Driving in Dense Traffic via Model-Predictive Policy Learning with Uncertainty Minimization}
\title{Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Mikael Henaff, Alfredo Canziani \& Yann LeCun \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
New York University\\
\texttt{\{mbh305\}@nyu.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
  Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training.
  In this work, we propose to train a policy while minimizing a measure of the mismatch between these two distributions.
  We obtain estimates of the distribution induced by the policy using a learned dynamics model, and propose a general, differentiable measure of the distribution mismatch using uncertainty estimates of the dynamics model's predictions, calculated using dropout. Our framework also naturally yields a model-based imitation learning algorithm when the observational data consists of expert trajectories.
  We evaluate both approaches using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction. 
\end{abstract}


\section{Introduction}

In recent years, model-free reinforcement learning methods using deep neural network controllers have proven effective on a wide range of tasks, from playing video or text-based games \citep{mnih15, A3C, NarasimhanKB15} to learning algorithms \citep{Zaremba15} and complex locomotion tasks \citep{Lillicrap2015, ZhangLMUC15}.
However, these methods often require a large number of interactions with the environment in order to learn.
While this is not a problem if the environment is simulated, it can limit the application of these methods in realistic environments where interactions with the environment are slow, expensive or potentially dangerous.
Building a simulator where the agent can safely try out policies without facing real consequences can mitigate this problem, but requires human engineering effort which increases with the complexity of the environment being modeled.


Model-based reinforcement learning approaches try to learn a model of the environment dynamics, and then use this model to plan actions or train a parameterized policy.
A common setting is where an agent alternates between collecting experience by executing actions using its current policy or dynamics model, and then using these experiences to improve its dynamics model.
This approach has been shown empirically to significantly reduce the required number of environment interactions \citep{Atkeson1997, PILCO, Nagabandi2017, Chua2018}.
%Theoretical results have also shown the model-based approach to offer improved sample complexity in simple settings \citep{LS-LQR, Recht2018}.

Despite these improvements in sample complexity, there exist settings where even a \textit{single} poor action executed by an agent in a real environment can have consequences which are not acceptable.
At the same time, with data collection becoming increasingly inexpensive, there are many settings where observational data of an environment is abundant.
This suggests a need for algorithms which can learn policies primarily from observational data, which can then perform well in a real environment.
Autonomous driving is an example of such a setting: on one hand, trajectories of human drivers can be easily collected using traffic cameras \citep{NGSIM}, resulting in an abundance of observational data; on the other hand, learning through interaction with the real environment is not a viable solution.


\begin{wrapfigure}{r}{0.3\textwidth}
%  \centering
  \includegraphics[width=0.3\textwidth]{figures/driving/uncertainty_example2-crop.pdf}
  \caption{\label{simple-example}Different models fitted on training points which cover a limited region the function's domain. Models make arbitrary predictions outside of this region.}
\end{wrapfigure}
However, learning policies from purely observational data is challenging because the data may only cover a small region of the space over which it is defined.
If the observational data consists of state-action pairs produced by an expert following an optimal policy, one option is to use imitation learning \citep{Pomerleau91}.
However, passive imitation learning is well-known to suffer from a mismatch between the states seen at training and execution time \citep{Ross2010EfficientRF}, and may need to be augmented by querying an expert \citep{Dagger}.
Another option is to learn a dynamics model from observational data, and then use it to train a policy. However, the dynamics model may make arbitrary predictions outside the domain it was trained on, which may wrongly be associated with low cost (or high reward) as shown in Figure \ref{simple-example}. The policy network may then exploit these errors in the dynamics model and produce actions which lead to these wrongly optimistic states.
In the interactive setting, this problem is naturally self-correcting, since states where the model predictions are wrongly optimistic will be more likely to be experienced, and thus will correct the dynamics model. However, the problem persists if the dataset of environment interactions which the model is trained on is fixed.


In this work, we propose to train a policy while explicitly penalizing the mismatch between the distribution of trajectories it induces and the one reflected in the training data.
%We do this by using a learned dynamics model to simulate trajectories produced by the policy network, which are then brought closer to the distribution of states seen during training through a differentiable uncertainty term.
We do this by using a learned dynamics model which is unrolled for multiple time steps, and training a policy network to minimize a differentiable cost over this rolled-out trajectory.
This differentiable cost contains two terms: a policy cost which represents the objective the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. 
We propose to measure this second cost by using the uncertainty of the dynamics model about its own predictions, calculated using dropout.
%We do this by first learning a dyanamics model, and then rolling out the forward model and training a policy network to minimize a differentiable cost over a rolled-out trajectory
%We propose to do this by adding a differentiable term to the cost function used to train the policy network, which represents some measure of uncertainty of the forward model about its own predictions, and minimize this over a relatively long time horizon.
%The key element of our approach is a differentiable term in the cost function used at planning time, representing the uncertainty of the forward model about its own predictions, which is calculated using dropout.
%Minimizing this term encourages a planner or policy network to choose action sequences which keep it on the data manifold which the forward model was trained on.
We evaluate our approach in the context of learning policies to drive an autonomous car in dense traffic, using a large-scale dataset of real-world driving trajectories which we also adapt into an environment for testing learned policies; both dataset and environment will be made public.
We show in our experiments that model-based control using this additional uncertainty regularizer substantially outperforms unregularized control, and enables learning good driving policies using only observational data with no environment interaction or additional labeling by an expert.


\begin{comment}
In this work, we propose an end-to-end, model-based approach for learning driving policies from purely observational data.
We first introduce a large-scale dataset of real-world driving trajectories, which we also adapt into an environment for testing learned policies and planning methods; both dataset and environment will be made public.
This dataset is highly stochastic due to the unpredictable nature of human driving, and we train both deterministic and stochastic action-conditional forward models which can then be used for planning.
The key element of our approach is a differentiable term in the cost function used at planning time, representing the uncertainty of the forward model about its own predictions, which is calculated using dropout.
Minimizing this term encourages a planner or policy network to choose action sequences which keep it on the data manifold which the forward model was trained on.
We show in our experiments that model-based control using this additional regularizer substantially outperforms unregularized control, as well as both standard and model-based imitation learners, and enables learning good driving policies using only observational data with no environment interaction.
\end{comment}

\section{Model-Predictive Policy Learning with Uncertainty Regularization}

We assume we are given a dataset of observational data which consists of state-action pairs $\mathcal{D} = \{(s_t, a_t)\}_t$.
We first describe our general approach, which consists of two steps: learning an action-conditional dynamics model using the collected observational data, and then using this model to train a fast, feedforward policy network which minimizes both a policy cost and an uncertainty cost. 

\subsection{Action-conditional Forward Model}

The dynamics model can be deterministic or stochastic.
A deterministic model $f_\theta(s_{1:t}, a_t)$ takes as input a sequence of observed or previously predicted states $s_{1:t}$ and an action $a_t$, and produces a prediction of the next state $\hat{s}_{t+1}$. The per-sample loss which it minimizes is given by $\ell(s_{t+1}, \hat{s}_{t+1})$, where $\ell$ is a loss function appropriate for the task at hand.
A stochastic model additionally takes as input a latent variable $z_t$ which represents the information about the next state $s_{t+1}$ which is not a deterministic function of the input.
In this work, we consider recent approaches for stochastic prediction based on Variational Autoencoders \citep{VAE, Babaeizadeh2018, Denton2018}, but other approaches could be used as well.
During training, latent variables are sampled from a posterior network $q_\phi(s_{1:t}, s_{t+1})$ which outputs the parameters $(\mu_\phi, \sigma_\phi)$ of a diagonal Gaussian distribution conditioned on the past inputs and true targets. This network is trained jointly with the rest of the model using the reparameterization trick, and a term is included in the loss to minimize the KL divergence between the posterior distribution and a fixed prior $p(z)$, which in our case is an isotropic Gaussian.
%Details of the stochastic model updates are given in Appendix \ref{model-details}. 
After training, different future predictions for a given sequence of frames can be generated by sampling different latent variables from the prior distribution.


%Our deterministic model $f_\theta(s_{1:t}, a_t)$ takes as input a sequence of past states $s_{1:t}$ and an action $a_t$, and produces a prediction of the next state $\hat{s}_{t+1} = (\tilde{i}_t, \tilde{u}_t)$.  
%The per-sample loss which it minimizes is given by:

%\begin{align}
%  \label{eq:update-eqn}
%  \ell(s_{1:t}, \hat{s}_{t+1}) = \|\tilde{i}_t - i_t \|_2^2 + \| \tilde{u}_t - u_t \|_2^2
%\end{align}



%After the dynamics model is trained, we train another network $f_c$ which takes as input a state predicted by the forward model and produces an estimate of the associated proximity and lane costs. This is trained by minimizing the per sample loss:

%\begin{align}
%  \|f_c(f_\theta(s_{1:t}, a_t, z_t)) - c_t \|_2
%\end{align}

%The deterministic and stochastic models have an identical architecture except for the latent variables in the stochastic model.
%Details of the prediction model and inference network architectures are given in Appendix \ref{model-details}.

\subsection{Training a Policy Network with Uncertainty Minimization}
\label{uncertainty-minimization}

Once the forward model is trained, we use it to train a parameterized policy network $\pi_\psi$, which we assume to be stochastic.
We first sample an initial state sequence $s_{1:t}$ from the training set, unroll the forward model over $T$ time steps, and backpropagate gradients of a differentiable objective function with respect to the parameters of the policy network.
During this process the weights of the forward model are fixed, and only the weights of the policy network are optimized.
This objective function contains two terms: a \textit{policy cost} $C$, which reflects the underlying objective the policy is trying to learn, and an \textit{uncertainty cost} $U$, which reflects how close the predicted state induced by the policy network is to the manifold which the data $\mathcal{D}$ is drawn from.
In the case of the deterministic model, training the policy involves solving the following problem:


    \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} C(\hat{s}_{t+i}) + \lambda U(\hat{s}_{t+1}) \Big],  \mbox{ such that: }
    \begin{cases}
      \hat{a}_{t+i} \sim \pi_\psi(s_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}) \\
      \end{cases}
    \end{align*}

%    In the case of the stochastic forward model, we additionally sample latent variables from the prior for each step and input them into the forward model. The problem them becomes:
    The stochastic forward model additionally takes as input latent variables sampled from the prior at each time step. The problem then becomes:
    
        \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} C(\hat{s}_{t+i}) + \lambda U(\hat{s}_{t+1}) \Big],  \mbox{ such that: }
    \begin{cases}
      z_{t+i} \sim p(z) \\
      \hat{a}_{t+i} \sim \pi_\psi(s_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}, z_{t+i}) \\
      \end{cases}
    \end{align*}

        The uncertainty cost $U$ is applied to states predicted by the forward model, and could reflect any measure of their likelihood under the distribution the training data is drawn from.
        We propose here a general form based on the uncertainty of the dynamics model, which is calculated using dropout.
        Intuitively, if the dynamics model is given a state-action pair from the same distribution as $\mathcal{D}$ (which it was trained on), it will have low uncertainty about its prediction.
        If it is given a state-action pair which is outside this distribution, it will have high uncertainty.

\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=0.7\textwidth]{figures/driving/svg-crop.pdf}} \\
    \label{svg}
    \caption{Training the policy network using the stochastic forward model. Gradients with respect to costs associated with predicted states are passed through the unrolled forward model into a policy network.}
\end{figure*}
        
        
    Dropout \citep{Dropout2012, Dropout2014} is a regularization technique which consists of randomly setting hidden units in a neural network to zero with some probability.
    The work of \citep{Gal16} showed that a neural network trained with dropout is equivalent to an approximation of a probabilistic deep Gaussian Process model.
    A key result of this is that estimates of the neural network's model uncertainty for a given input can be obtained by calculating the covariance of its outputs taken over multiple dropout masks.
    We note that this uncertainty estimate is the composition of differentiable functions: each of the models induced by applying a different dropout mask is differentiable, as is the covariance operator.
    Furthermore, we can summarize the covariance matrix by taking its trace (which is equal to the sum of its eigenvalues, or equivalently the sum of the variances of the outputs across each dimension), which is also a differentiable operation. This provides a scalar estimate of uncertainty which is differentiable with respect to the input.

    More precisely, let $f_{\theta_1}, ..., f_{\theta_K}$ denote our prediction model with $K$ different dropout masks applied to its hidden units (this can also be viewed as changing its weights). We define our scalar measure of uncertainty $\Omega$ as follows:
%    Then for some set of inputs $(s_{1:t}, a_t, z_t)$, the uncertainty of the forward model is given by the covariance matrix of the outputs under the different dropout masks (up to some constants).

%    \begin{align}
%      \mbox{Cov} [\{ f_{\theta_k}(s_{1:t}, a_t, z_t) \}_{k=1}^K]
%    \end{align}


    \begin{align*}
      \Omega(s_{1:t}, a_t, z_t) &= \mbox{tr} \Big[ \mbox{Cov} [\{ f_{\theta_k}(s_{1:t}, a_t, z_t) \}_{k=1}^K] \Big] \\
      &= \sum_{j=1}^d \mbox{Var}(\{ f_{\theta_k}(s_{1:t}, a_t, z_t)_j \}_{k=1}^K)
    \end{align*}


    %This allows us to obtain gradients of the inputs, and specifically actions, with respect to this measure of model uncertainty, which gives us directions in action space which would make the model more certain.

\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=0.8\textwidth]{figures/driving/ep_uncertainty_cost-crop.pdf}} \\
    \label{planning-methods}
    \caption{Training the policy network using the differentiable uncertainty cost, calculated using dropout.}
\end{figure*}




where $d$ is the dimensionality of the output. Minimizing this quantity with respect to actions encourages the policy network to produce actions which, when plugged into the forward model, will produce predictions which the forward model is confident about.
    To compensate for differences in baseline uncertainty across different modalities or rollout lengths, we estimate the empirical mean and variance of $\Omega$ for every rollout length $t$ of the forward model over the training set, to obtain $\mu_{\Omega}^t$ and $\sigma_{\Omega}^t$. We then define our uncertainty cost as follows:

    \begin{align}
      U(\hat{s}_{t+1}) = U(s_{1:t}, a_t, z_t) = \Big [ \frac{\Omega(s_{1:t}, a_t, z_t) - \mu_\Omega^t}{\sigma_\Omega^t} \Big]_+
    \end{align}

    If the uncertainty estimate is lower than the mean uncertainty estimate on the training set for this rollout length, this loss will be zero.
    These are cases where the model prediction is within normal uncertainty ranges. If the uncertainty estimate is higher, this loss exerts a pull to change the action so that the future state will be predicted with higher confidence by the forward model.



A simple way to define $U$ given an initial sequence of states $s_{1:t}$ from $\mathcal{D}$ would be to set $U(\hat{s}_{t+k}) = \|\hat{s}_{t+k} - s_{t+k}\|$.
This would encourage the policy network to output actions which lead to a similar trajectory as the one observed in the dataset.
Although this leads to a set of states the model is presumably confident about, it may not be a trajectory which also satisfies the policy cost $\textit{C}$.
The advantage of using the more general cost above is that, assuming the dynamics model generalizes well within its training distribution, there will be multiple trajectories which it will be confident about, and the policy network can then choose one which best satisfies the cost it is trying to optimize.
However, if the dataset $\mathcal{D}$ consists of \textit{expert} trajectories, a natural choice would be to set $C(\hat{s}_{t+i}) = U(\hat{s}_{t+i}) = \frac{1}{2} \|\hat{s}_{t+k} - s_{t+k}\|$. This give a model-based imitation learning objective which simultaneously optimizes the policy cost and the uncertainty cost.


%    For the deterministic model, our approach is similar to that of \citep{Nguyen1989}.
%    For the stochastic model, our approach is related to the $\mbox{SVG}(\infty)$ algorithm introduced in \citep{SVG}, with two differences. The first is that they sample a sequence of states $s_t, ..., s_{t+T}$ from the training set (or replay buffer), and infer the corresponding sequence of latent variables $z_t, ..., z_{t+T}$ (in our setup, this can be done using the posterior network $q_\phi$), whereas we sample latent variables from the prior. We compare the two approaches and found that using inferred latent variables causes a drop in performance, which we discuss in Section \ref{experiments}.
%    The second is that they consider an interactive setting where the agent collects experience from the environment which it then uses to train its dynamics model.

\begin{comment}
\subsection{Model-Based Imitation Learning}

  We also experimented with a variant of imitation learning using the learned stochastic model, which we found performed better than the standard version. One issue with imitation learning is that errors can accumulate quickly, leading to divergence from the states seen during training.
  One cause may be that the model is simply minimizing the $\ell_2$ loss in action space, which may not correspond to minimizing distances in trajectory space.
  Consider the following example where an agent is walking exactly along the side of a cliff, and must output an action which represents its angle.
  To the left is a drop, and to the right is solid ground.
  Say the expert action is to go straight, i.e. $a_{t+1} = 0$. Now consider two possible actions predicted by the network, $\hat{a}_{t+1} = -\epsilon$ (slight left) and $\hat{a}_{t+1} = +\epsilon$ (slight right).
  Both of these actions incur the same $\ell_2$ cost of $\epsilon$, but have very different consequences. If the agent moves slightly away from the expert action on the left side, they fall off the cliff, which causes a large deviation of their subsequent states from the expert states. If they move slightly to the right however, they stay on solid ground and their subsequent states remain close to the expert states.

  As an alternative, we experimented with training a policy to match expert \textit{trajectories}, rather than actions.
  We do this by unrolling the forward model for multiple timesteps, outputting actions by the policy network using the model predictions, and minimizing the error between the final trajectory output by the forward model and the expert trajectory observed in the dataset.
%  This is illustrated in Figure \ref{mbil-diagram}.
  The motivation here is that if the policy network outputs an action which causes small divergence from the target trajectory at the next timestep, but large divergences later on, it will receive gradients from these larger errors backpropagated through the unrolled forward model.
\end{comment}


\section{Dataset and Planning Environment}
\label{dataset-and-planning}

We apply our approach to learn driving policies using a large-scale dataset of driving videos taken from traffic cameras.
%To begin with, we summarize the dataset and planning environment which we used, with full details provided in Appendix \ref{i80-dataset-prep}.
The Next Generation Simulation program's Interstate 80 (NGSIM I-80) dataset \citep{NGSIM} consists of 45 minutes of recordings from traffic cameras mounted over a stretch of highway.
The driver behavior is complex and includes sudden accelerations, lane changes and merges which are difficult to predict; as such the dataset has high enviroment stochasticity.
After recording, a viewpoint transformation is applied to rectify the perspective, and vehicles are identified and tracked throughout the video.
This yields a total 5596 car trajectories, which we split into training ($80\%$), validation ($10\%$) and testing sets ($10\%$).

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/driving/I-80-blue-crop.pdf}
  \caption{
    Preprocessing pipeline for the NGSIM-I80 data set.
    Orange arrows show same vehicles across stages.
    Blue arrows show corresponding extracted context state.
    (a) Snapshots from two of the seven cameras.
    (b) View point transformation, car localisation and tracking.
    (c) Context states are extracted from rectangular regions surrounding each vehicle.
    (d) Five examples of context states extracted at the previous stage.
  }
\label{I-80}
\end{figure}


We then applied additional preprocessing to obtain a state and action representations $(s_t, a_t)$ for each car, suitable for learning an action-conditional predictive model.
Our state representation $s_t$ consists of two components: an image $i_t$ representing the neighborhood of the car, and a vector $u_t$ representing its current position and velocity.
The images $i_t$ are centered around the ego car and encode both the lane emplacements and the locations of other cars.
Each image has 3 channels: the first (red) encodes the lane markings, the second (green) encodes the locations of neighboring cars, which are represented as rectangles reflecting the dimensions of each car, and the third channel (blue) represents the ego car, also scaled to the correct dimensions.
This is summarized in Figure \ref{I-80}.
We also define two cost functions which together make up the policy cost: a proximity cost which reflects how close the ego car is to neighboring cars, and a lane cost which reflects how much the ego car overlaps with lane markings. These are represented as a cost vector at each timestep, $c_t = (C_{proximity}(s_t), C_{lane}(s_t))$. Full details can be found in Appendix \ref{i80-dataset-prep}. 


We also adapted this dataset to be used as an environment to evaluate learned policies, with the same interface as OpenAI Gym \citep{OpenAIBaselines}.
Choosing a policy for neighboring cars is challenging due to a cold-start problem: to accurately evaluate a learned policy, the other cars would need to follow human-like policies which would realistically react to the controlled car, which are not available.
We take the approach of letting all the other cars in the environment follow their trajectories from the dataset, while a single car is controlled by the policy we seek to evaluate.
This approach avoids hand-designing a policy for the neighboring cars which would likely not reflect the diverse nature of human driving.
The limitation is that the neighboring cars do not react to the controlled car, which likely makes the problem more difficult as they do not try to avoid collisions.

  
\section{Related Work}

A number of authors have explored the use of learned, action-conditional forward models which are then used for planning, starting with classic works in the 90's \citep{Nguyen1990, Schmidhuber1990, Jordan1992}, and more recently in the context of video games \citep{Oh15, Pascanu17, I2A}, robotics and continous control \citep{FinnGL16, Poke, Nagabandi2017, UPN}.
Our approach to learning policies by backpropagating through a learned forward model is related to the early work of \citep{Nguyen1989} in the deterministic case, and the SVG framework of \citep{SVG} in the stochastic case. However, neither of these approaches incorporates a term penalizing the uncertainty of the forward model when training the policy network.

The works of \citep{DeepPilco, Chua2018} also used model uncertainty estimates calculated using dropout in the context of model-based reinforcement learning. They did so during the forward prediction step. Namely, they used different dropout masks to simulate different state trajectories which were then averaged to produce a cost estimate used to select an action.

Our model uncertainty penalty is related to the cost used in \citep{Kahn2017}, who used dropout and model ensembling to compute uncertainty estimates for a binary action-conditional collision detector for a flying drone. These estimates were then used to select actions out of a predefined set which yielded a good tradeoff between speed, predicted chance of collision and uncertainty about the prediction. In our work, we apply uncertainty estimates to the predicted high-dimensional states of a forward model at every time step, summarize them into a scalar, and backpropagate gradients through the unrolled forward model to then train a policy network by gradient descent.


The problem of covariate shift when executing a policy learned from observational data has been well-recognized in imitation learning.
It was first noted in the early work of \citep{Pomerleau91}, and was shown in \citep{Ross2010EfficientRF} to cause a regret bound which grows quadratically in the time horizon of the task.
The work of \citep{Dagger} proposed a method to efficiently use expert feedback if available, which has also been applied in the context of autonomous driving \citep{Zhang16}.
Our approach also addresses covariate shift, but does so without querying an expert.

Our model-based imitation learning algorithm is related to the work of \citep{Englert2013}, who also performed imitation learning at the level of trajectories rather than individual actions. They did so in low-dimensional settings using Gaussian Processes, whereas our method uses an unrolled neural network representing the environment dynamics which can be applied to high-dimensional state representations. The work of \citep{Baram2017EndtoEndDA} also used a dynamics model in the context of imitation learning, but did so in the interactive setting to minimize a loss produced by a discriminator network. 


Several works have applied deep learning methods in the context of autonomous driving. The works of \citep{Pomerleau91, Muller2006, Bojarski16, Pan17} used neural networks to control policies trained by imitation learning, while \citep{Williams2017} learned models of the vehicle dynamics. These works focused on lane following or offroad driving in visually rich environments and did not focus on settings with dense traffic, which we focus on in this work. The work of \citep{Sadigh16} developed a model of the interactions between the two drivers which was then used to plan actions in simple settings, using symbolic state representations. In our work, we consider the problem of learning driving policies in dense traffic, using high-dimensional state representations which reflect the neighborhood of the ego car. %The work of \citep{Zhang16} also considered a setting with other cars, and used imitation learning together with an efficient method of querying the actions of an expert to


%Our model-based imitation learning approach is related to the work of \citep{Englert2013}, who also performed imitation learning at the level of trajectories rather than actions.
%They worked with low-dimensional state vectors representing real robots arms, which allowed them to use Gaussian Processes, and the primary source of uncertainty was measurement error. Gaussian Processes are highly data efficient, but do not scale well to high dimensions. In our setting, states include high-dimensional images and we are trying to model uncertain behavior of other drivers. The work of \citep{baram17} also used an unrolled forward model for imitation learning, but did so in the context of Generative Adversarial Imitation Learning and in deterministic environments.

%Our approach to learning policies which minimize predicted costs fits within the conceptual framework of Stochastic Value Gradients (SVG) \citep{SVG}, and extends it to a setting with high-dimensional state representations.
%This requires us to use more sophisticated stochastic models than the ones in the original work, which used additive Gaussian noise whose parameters were learned using the reparamaterization trick.
%They also considered an online setting where the agent continues to collect experience, which reduces the need for the epistemic uncertainty penalty, whereas we found this to be essential in our setting where we learn purely from observational data.




\section{Experiments}
\label{experiments}

We now report experimental results. We designed a deterministic and stochastic forward model to model the state and action representations described in Section \ref{dataset-and-planning}, using convolutional layers to process the images $i_t$ and fully-connected layers to process the vectors $u_t$ and actions $a_t$. All model details can be found in Appendix \ref{model-details} and training details can be found in Appendix \ref{training-details-appendix}.


    \subsection{Prediction Results}


    We first generated predictions using both deterministic and stochastic forward models, shown in  Figure \ref{prediction-results}.
    The deterministic model produces predictions which become increasingly blurry further into the future. Our stochastic model produces predictions which stay sharp far into the future.
    By sampling different sequences of latent variables, different future scenarios are generated.
    Note that the two sequences generated by the stochastic model are different from the ground truth future which occurs in the dataset.
    This is normal as the future observed in the dataset is only one of many possible ones.
    Additional video generations can be viewed at the following URL: \url{https://youtu.be/wRrQEvLq3dA}.



\begin{figure*}[t!]
    \centering
    \subfigure[Ground truth sequence]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/truth-crop.pdf}}
    \subfigure[Deterministic Model]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/det-crop.pdf}} \\
    \subfigure[Stochastic Model, sample 1]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/pred1-crop.pdf}}
    \subfigure[Stochastic Model, sample 2]{\includegraphics[width=0.48\textwidth]{figures/driving/video_predictions/pred2-crop.pdf}} \\
    \label{prediction-results}
    \caption{Video prediction results using a deterministic and stochastic model. Two different future predictions are generated by the stochastic model by sampling two different sequences of latent variables. The deterministic model averages over possible futures, producing blurred predictions.}
\end{figure*}









    \subsection{Policy Evaluation Results}

    We evaluated different policies using two measures: whether the controlled car reaches the end of the road segment without colliding into another car or driving off the road, and the distance travelled before the episode ends. Policies which collide quickly will travel shorter distances. All cars are initialized at the beginning of the road segment with the initial speed they were driving at in the dataset, and then are controlled by the policy being measured. We only report performance for cars in the testing trajectories, which were not used when training the forward model or policy network.

    We compared our approach against several baselines which can also learn from observational data, which we briefly describe below. Full details of each of them are contained in Appendix \ref{planning-methods}.
    All policy networks have the same architecture, and all are fed the concatenation of the 20 previous states as input. They all output the parameters of a 2D diagonal Gaussian over action space, from which the next action is sampled.

    \textbf{Human:} The actual human trajectories observed in the testing set, which are all collision-free.
    
    \textbf{No action:} A policy which outputs an action of zero, maintaining its current speed and direction.

%    \textbf{Rule-Based Policy:} A hardcoded policy which outputs an action which moves the car in the opposite direction from the closest neighboring car at the current time step.
    \textbf{1-step Imitation Learner:} A policy network trained to minimize the negative log-likelihood of the next human action observed in the dataset under the parameters of its output distribution.
    
    \textbf{SVG$(\infty)$}: A policy network trained with stochastic value gradients, using our stochastic forward model. Here the latent variables are inferred using the posterior network for a state trajectory sampled from the training set. This is the same setup as \citep{SVG}, with the difference that the agent does not interact with the environment and learns from a fixed observational dataset.
    
    \textbf{VG$(\infty)$}: This is the same setup as SVG($\infty$), but using the deterministic forward model, which is closely related to that of \citep{Nguyen1989}.  







    The policy cost which we minimize for VG, SVG and MPPLUM-D and SPPLUM-D is given by:

    \begin{equation}
      C = C_{proximity} + 0.2 \cdot C_{lane}
    \end{equation}

    where $C_{proximity}$ and $C_{lane}$ are the proximity and lane costs described in Section \ref{dataset-and-planning}. This puts a higher priority on avoiding other cars while still encouraging the policy to stay within the lanes. MMPLUM-D additionally minimizes $U$, the model uncertainty cost described in Section \ref{uncertainty-minimization}. We set $\lambda=0.01$ for the deterministic model and $\lambda=0.05$ for the stochastic model. 


\begin{figure*}[t!]
    \centering
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/driving/distance_perf-crop.pdf}}
    \subfigure[]{\includegraphics[width=0.48\textwidth]{figures/driving/success_perf-crop.pdf}}
    \label{performance}
    \caption{Performance on NGSIM dataset.}
\end{figure*}
    


\begin{comment}
  \begin{table}[t]
    \centering
  \begin{tabular}{|lrr|}
    \hline
    Method & Mean Distance & Success Rate (\%)  \\
    \hhline{|===|}
    Human & 1358 & 100.0 \\
    \hline
    No action & 566 & 16.2 \\
    Rule-Based Policy & - & - \\
    \hline
    1-step IL & 322 & 1.4 \\
    MBIL (1 step) & 327 & 6.4 \\
    MBIL (3 step) & 473 & 16.2 \\
    MBIL (5 step) & 830 & 37.5 \\
    MBIL (10 step) & 943 & 67.1 \\
    MBIL (20 step) & 1024 & 66.6 \\
    MBIL (30 step) & 1028 & 67.7 \\
    Stochastic MBIL (10 step) & 284 & 0.7 \\
    Stochastic MBIL (20 step) & 333 & 1.2 \\
    Stochastic MBIL (30 step) & 349 & 0.7 \\
    Stochastic MBIL (40 step) & 386 & 0.8 \\
    \hline
    VG($\infty$) & 97 & 0.0 \\
    SVG($\infty$) & 88 & 0.0 \\
    VG($\infty$) + Epistemic Cost & 963 & 56.4 \\
%    SVG($\infty$) & 103.0 & 0.0 \\
%    SVG($\infty$) + Epistemic Cost & 525.8 & 14.5 \\
    SVG($\infty$) + Epistemic Cost & \textbf{976} & \textbf{59.1} \\
    \hline
  \end{tabular}
  \caption{Planning performance on NGSIM dataset.}
  \label{main-table}
  \end{table}

\end{comment}


Figure \ref{performance} compares performance for the different methods.
The 1-step imitation learner, SVG and VG all perform poorly, and do not beat the simple baseline of performing no action.
MPPLUM-D and MMPLUM-I both perform increasingly well with longer rollouts.
MMPLUM-I performs similarly to the 1-step imitation learner when given a rollout length of 1.
This makes sense as they both optimize similar objectives. 
%: the 1-step IL optimizes the match between its action and the expert action, and the 1-step MPPLUM-I optimizes the match between the next state predicted by the forward model for the action it produces and the true next state in the expert trajectory.
Training on longer rollouts is essential, as it enforces a match between the distribution of states the policy induces and the expert's distribution over longer time horizons.

Videos of the learned policies for both MPPLUM-I and MPPLUM-D driving in the environment can be found at \url{url}.
The policy learns effective behaviors such as braking, accelerating and turning to avoid other cars.
Figure \ref{trajectories} shows trajectories on the map for different methods. We see that the single-step imitation learner produces divergent trajectories which turn into other lanes, whereas the MPPLUM methods show trajectories which primarily stay on their lanes. 

MPPLUM-D become equivalent to VG if we remove the uncertainty penalty, and the large difference in performance shows that this is essential.
Table \ref{svg-pred} shows the average predicted policy cost and uncertainty cost of the two methods.
VG produces much lower predicted policy cost, yet very high uncertainty cost. This indicates that the actions the policy produces induce a distribution over states which the forward model is highly uncertain about. The policy trained with SMMPLUM-D produces higher policy cost estimates, but lower uncertainty cost.

A somewhat surprising finding is that the stochastic model only yields a modest improvement over the deterministic model in the case of SPPLUM-D, despite the improvement in terms of visual quality shown in Figure \ref{prediction-results}, and gives much \textit{worse} performance for SMPPLUM-I.
On further investigation, we found that the forward model becomes less responsive to input actions when it is fed latent variables inferred by the posterior network, as opposed to latent variables which are sampled from the prior. This is shown in Appendix \ref{action-sensitivity-appendix} by comparing predictions of the forward model conditioned on different action sequences, using latent variables inferred or sampled from the prior. We also trained a policy network using MPPLUM-D with inferred rather than sampled latent variables, and found a large drop in performance.
One possible explanation is that the forward model encodes some factors of variation of the output due to the actions in its latent variables.
The sequence of latent variables sampled from the prior are independent, which may cause these effects to cancel each other over the sequence. However, the latent variables inferred from a sequence of states in the training set are highly dependent, and together they may explain away the effects of the actions.
This suggests that enforcing independence between actions and latent variables may be necessary to leverage the benefits of stochastic models.





\begin{figure*}[t!]
    \centering
    \subfigure{\includegraphics[width=0.48\textwidth]{figures/driving/trajectories-crop.pdf}}
    \label{trajectories}
    \caption{Trajectories}
\end{figure*}
  


%  To illustrate the effect of the epistemic uncertainty cost, we plot predictions made by the stochastic forward model for an action sequence produced by SVG policy networks trained with and without the epistemic uncertainty cost, shown in Figure \ref{svg-pred}.
%  The policy network trained with the uncertainty cost produces actions which, when plugged into the forward model, yield predictions which remain on the data manifold.
%  The policy network trained without the uncertainty cost, however, produces actions which produce predictions not resembling any of the training examples.
%  Notice however that these predictions yield low proximity cost, since the green channel representing cars is almost zero.
%  Also included in Figure \ref{svg-pred} are the average predicted proximity cost by the forward model for both policies, as well as average epistemic uncertainty cost $C_U$.
%  The model trained without the uncertainty penalty produces actions for which the forward model predicts very low cost, but its uncertainty about its predictions is very high.
%  The model trained with the penalty produces actions for which the forward model predicts higher cost, but with much less uncertainty.
%  Including the uncertainty cost forces the policy network to produce actions which still produce reasonable predictions under the forward model, and give much better results when executed in the environment.

\begin{figure*}[t!]
    \centering
%    \subfigure[with uncertainty cost]{\includegraphics[width=0.48\textwidth]{figures/driving/svg_reg_prediction-crop.pdf}}
%    \subfigure[without uncertainty cost]{\includegraphics[width=0.48\textwidth]{figures/driving/svg_no_reg_prediction-crop.pdf}} \\
    \subfigure{

  \begin{tabular}{|lrrrr|}
    \hline
    Method & Mean Distance & Success Rate & Total Predicted Cost & $U$ \\
    \hhline{|=====|}
    SVG($\infty$) & 88.7 & 0.0 & 0.03 & 4087.4 \\
    MPPLUM-D & 976.4 & 59.1 & 0.22 & 1.1 \\
    \hline
  \end{tabular}
    }
%  Executing these actions in the environment gives very poor performance, as shown in Table \ref{main-table}.
    \caption{Predictions made by the same forward model for an action sequence produced by a policy trained \textit{with} (top) and \textit{without} (bottom) the epistemic uncertainty cost. Without the uncertainty cost, the policy network learns to output pathological action sequences which produce invalid predictions which nevertheless have low cost.}
    \label{svg-pred}
\end{figure*}

\begin{comment}
There is a significant difference in performance between $\mbox{SVG}(\infty)$ and $\mbox{SVG-sim}(\infty)$.
The main difference between these two methods is that $\mbox{SVG}(\infty)$ uses latent variables which are inferred from true trajectories in the training set using the posterior network, whereas $\mbox{SVG-sim}(\infty)$ uses latent variables which are sampled from the prior distribution $p(z)$. To better understand the effect of these two approaches, we compared the predictions made by the forward model using inferred and sampled latent variables for different sequences of actions sampled from the training set. Examples are shown in Figure \ref{action-sensitivity}. The predicted sequences using the inferred latent variables resemble the ground truth sequence, even when changing the sequence of actions input to the forward model. In contrast, when using latent variables sampled from the prior, changing the action sequence results in different sequences being predicted by the forward model.
This suggests that using inferred latent variables reduces the sensitivity of the forward model to the actions.
To quantify this, we measured the magnitude of the partial derivatives of the output images with respect to actions, averaged over the training set, and found that these derivative were much smaller when the latent variables were inferred rather than sampled.
\end{comment}

\begin{figure*}[t!]
    \centering
  \begin{tabular}{|ccc|}
    \hline
    Method & $\| \partial i_{t+1} / \partial a_t \|_2$ & $\| \partial u_{t+1} / \partial {a_t} \|_2$\\
    \hhline{|===|}
    SVG($\infty$) & - & - \\
    SVG($\infty$) & - & - \\
    \hline
  \end{tabular}
\end{figure*}




%We ran an additional experiment where we trained a forward model which samples the latent variables from the prior, rather than the posterior, with probability $0.5$, in an effort to encourage it to extract more information from the actions.

%  We additionally performed several ablation experiments to understand the effects of different modeling choices, shown in table \ref{ablation}.
%  Using the action buffer has a non-negligible effect on the SMPC planning, as we see a performance drop when starting from newly initialized actions every time planning is required rather than initializing with a previously optimized action sequence.
%  It is possible that with larger numbers of gradient steps this difference in performance would decrease, but the current setup is already very expensive.
%  We also compared our TEN model to a VAE model with the same architecture and hyperparameters, except for the $\beta$ hyperparameter representing the weight of the KL term in the loss, which we set to $10^{-6}$ (we optimized over the range $\{ 1, 10^{-1},..., 10^{-6} \}$ and found that values higher than $10^{-5}$ produced blurry predictions similar to the deterministic model.
%  Performance is very similar for both models.

%  Finally, we compare SVG performance with and without masking the $z$ variables during training.
%  Not including the masking causes a performance drop due to the forward model becoming less sensitive to the actions.


  \section{Conclusion}

  \textcolor{red}{TODO: improve}
  
  In this work we have presented an end-to-end approach for learning driving policies from observational data, which includes preparing a dataset of real-world driving trajectories, adapting it to become a planning environment, training action-conditional forward models, and using them to train policies using with a new uncertainty regularizer which addresses the domain mismatch problem.
  There are several directions for future work.
  Although we have applied it here in the context of learning driving policies, the approach is general and could be used in other domains.
  Furthermore, our current approach does not capture dependencies which are longer than 20 time steps into the future, which corresponds to 2 seconds.
  We tried two different approaches to capture longer-term dependencies: learning a value function using temporal differences (which in theory can capture arbitrary length dependencies) and unrolling for more time steps, but these did not yield any improvements and sometimes hurt performance. This requires more investigation.
%  Second, we observed that the latent variables still encoded action information, although adding the dropout on the latent variables reduced this problem.
%  Finding a more principled method would be an interesting direction for future work.
  Second, it would be interesting to optimize actions or policies to produce more complex and useful behaviors, such as changing to a specified lane while avoiding other cars.
  In the current setup, the policies are optimized only to avoid collisions and stay within lanes when possible, whereas in a real-world scenario we would want a policy which can safely navigate among traffic to different locations.



\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.

\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\appendix

\section{Dataset and Planning Environment}
\label{i80-dataset-prep}

To begin with, we describe the details and preparation of the dataset and planning environment which we used, which are summarized in Figure \ref{I-80}.
The Next Generation Simulation program's Interstate 80 (NGSIM I-80) dataset \citep{NGSIM} consists of 45 minutes of recordings made of a stretch of highway in the San Francisco Bay Area by cameras mounted on a 30-story building overlooking the highway. The recorded area includes six freeway lanes (including a high-occupancy vehicle lane) and an onramp.
The driver behavior is complex and includes sudden accelerations, lane changes and merges which are difficult to predict; as such the dataset has high aleatoric uncertainty.
There are three time segments, each of 15 minutes, taken at different times of day which capture the transition between uncongested and congested peak period conditions.
After recording, a viewpoint transformation is applied to rectify the perspective, and vehicles are identified and tracked throughout the video; additionally, their size is inferred.
This yields a total 5596 car trajectories, represented as sequences of coordinates $\{x_t, y_t\}$. We split these trajectories into training ($80\%$), validation ($10\%$) and testing sets ($10\%$).

We then applied additional preprocessing to obtain suitable representations for learning a predictive model.
Specifically, we extracted the following: i) a state representation for each car at each time step $s_t$, which encodes the necessary information to choose an action to take, ii) an action $a_t$ which represents the action of the driver, and iii) a cost $c_t$, which associates a quality measure to each state. We describe each of these below.


\textbf{State representation}:
Our state representation consists of two components: an image representing the neighborhood of the car, and a vector representing its current position and velocity.
For the images, we rendered images centered around each car which encoded both the lane emplacements and the locations of other cars.
Each image has 3 channels: the first (red) encodes the lane markings, the second (green) encodes the locations of neighboring cars, which are represented as rectangles reflecting the dimensions of each car, and the third channel (blue) represents the ego car, also scaled to the correct dimensions.
All images have dimensions $3 \times 117 \times 24$, and are denoted by $i_t$.
\footnote{Another possibility would have been to construct feature vectors directly containing the exact coordinates of neighboring cars, however this presents several difficulties.
First, cars can enter and exit the neighborhood, and so the feature vector representing the neighboring cars would either have to be dynamically resized or padded with placeholder values.
Second, this representation would not be permutation-invariant, and it is unclear where to place a new car entering the frame.
Third, encoding the lane information in vector form would require a parametric representation of the lanes, which is more complicated.
Using images representations naturally avoids all of these difficulties.}
Two examples are shown in Figure \ref{cost}.
We also computed vectors $u_t = (p_t, \Delta p_t)$, where $p_t = (x_t, y_t)$ is the position at time $t$ and $\Delta p_t = (x_{t+1} - x_t, y_{t+1} - y_t)$ is the velocity.


\begin{figure}
  \centering
  \subfigure[19.8 km/h]{
  \includegraphics[height=0.3\textheight]{figures/driving/image_198-crop.pdf}
  \includegraphics[height=0.3\textheight]{figures/driving/mask_198-crop.pdf}
  }
  \hspace{15mm}
  \subfigure[50.3 km/h]{
  \includegraphics[height=0.3\textheight]{figures/driving/image_503-crop.pdf}
  \includegraphics[height=0.3\textheight]{figures/driving/mask_503-crop.pdf}
  }
  \caption{Image state representations and proximity cost masks for cars going at different speeds. The higher the speed, the longer the safety distance required to maintain low cost.}
\label{cost}
\end{figure}







%Every vehicle in the simulator is initialised with starting position $\bm{p} = (x, y)$, initial velocity $v \bm{d}$ (where $v$ represents the speed and $\bm{d}$ the direction versor), correct dimensions, and identification (ID) number.
%At every time step $\Delta t = 0.1\,\text{s}$, we use the tracked trajectories (previously smoothed with a $1.5\,\text{s}$ running average) and a simplified kinematic model of a car to compute the agent actions $(a, b)$, corresponding to acceleration and tangential deviation.
%Therefore, the vehicle internal state is updated with: $\bm{p} \gets \bm{p} + v \bm{d} \Delta t$, $v \gets v + a \Delta t$, $\bm{d} \gets \bm{d} + v b \bm{d}_\perp \Delta t$, and $\bm{d} \gets \bm{d} / \lVert \bm{d} \rVert$.
%A longitudinal and transverse inter-vehicle linear proximity cost is computed, which is maximum in case of collision and goes to zero if vehicles are sufficiently spaced.
%Finally, the vehicles are drawn onto the screen canvas and displayed with their corresponding ID number.
%\textbf{(d)} Lanes and vehicles are also drawn on a secondary canvas, from which the context states are extracted.
%Rectangular regions around each vehicle --- oriented according to $\bm{d}$, of length corresponding to twice the space travelled in $1\,\text{s}$ at $130\,\text{km/h}$, of width equal four times the lane width --- scaled by a $0.5$ factor constitute the context states.
%    At this stage, the lane-crossing cost is computed using a modified morphological distance transform between each vehicle and the lane channel.



\textbf{Action representation}: Each action vector $a_t$ consists of two components: an acceleration (which can be positive or negative) which reflects the change in speed, and a change in angle.
The acceleration at a given time step is computed by taking the difference between two consecutive speeds, while the change in angle is computed by projecting the change in speed along its orthogonal direction:

\begin{align*}
  \Delta \mbox{speed} &= \| \Delta p_{t+1} \|_2 - \| \Delta p_t \|_2 \\
  \Delta \mbox{angle} &= (\Delta p_{t+1} - \Delta p_t)^\top (\Delta p_t)_\perp / \| \Delta p_t \|_2  \\
  a_t &= (\Delta \mbox{speed}, \Delta \mbox{angle}) \\
\end{align*}




\textbf{Cost}: Our cost function has two terms: a proximity cost and a lane cost. The proximity cost reflects how close the ego car is to neighboring cars, and is computed using a mask in pixel space whose width is equal to the width of a lane and whose height depends on the speed of the car. Two examples are shown in Figure \ref{cost}.
This mask is pointwise multiplied with the green channel, and the maximum value is taken to produce a scalar cost.
The lane cost uses a similar mask fixed to the size of the car, and is similarly multiplied with the red channel, thus measuring the car's overlap with the lane.
Both of these operations are differentiable so that we can backpropagate gradients with respect to these costs through images predicted by a forward model.

This preprocessing yields a set of state-action pairs $(s_t, a_t)$ (with $s_t=(i_t, u_t)$) for each car, which constitute the dataset we used for training our prediction model.
We then use the cost function to optimize action sequences at planning time, using different methods which we describe in Section \ref{planning-methods}.

We now describe how we adapted this dataset to be used as an environment to evaluate planning methods.
Building an environment for evaluating policies for autonomous driving is not obvious as it suffers from a cold-start problem.
Precisely measuring the performance of a given driving policy would require it to be evaluated in an environment where all other cars follow policies which accurately reflect human behavior.
This involves reacting appropriately both to other cars in the environment as well as the car being controlled by the policy being evaluated.
However, constructing such an environment is not possible as it would require us to already have access to a policy which drives as humans do, which in some sense is our goal in the first place. One could hand-code a driving policy to control the other cars in the environment, however is it not clear how to do so in a way which accurately reflects the diverse and often unpredictable nature of human driving.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/driving/traffic_gym_code-crop.pdf}
  \caption{NGSIM planning environment.}
\label{interface}
\end{figure}


We adopt a different approach where we let all other cars in the environment follow their trajectories in the dataset, while controlling one car with the policy we seek to evaluate.
The trajectory of the controlled car is updated as a function of the actions output by the policy, while the trajectories of the other cars remain fixed.
If the controlled car collides with another car, this is recorded and the episode ends.
This approach has the advantage that all other cars in the environment maintain behavior which is close to human-like.
The one difference with true human behavior is that the other cars do not react to the car being controlled or try to avoid it, which may cause crashes which would not occur in real life.
The driving task is thus possibly made more challenging than in a true environment, which we believe is preferable to using a hand-coded policy.
The interface is set up the same way as environments in OpenAI Gym \citep{OpenAIBaselines}, and can be accessed with a few lines of Python code, as shown in Figure \ref{interface}.



\section{Model Details}
\label{model-details}

The architecture of our forward model consists of four neural networks: a state encoder $f_\text{enc}$, an action encoder $f_\text{act}$, a decoder $f_\text{dec}$, and the posterior network $f_\phi$.
At every time step, the state encoder takes as input the concatenation of 20 previous states, each of which consists of a context image $i_t$ and a 4-dimensional vector $u_t$ encoding the car's position and velocity.
The images $i_{t-20}, ..., i_t$ are run through a 3-layer convolutional network with 64-128-256 feature maps, and the vectors $u_{t-20}, ..., u_t$ are run through a 2-layer fully connected network with 256 hidden units, whose final layers contain the same number of hidden units as the number of elements in the output of the convolutional network (we will call this number $n_H$).
The posterior network takes the same input as the encoder network, as well as the the ground truth state $s_{t+1}$, and maps them to a distribution over latent variables, from which one sample $z_t$ is drawn.
This is then passed through an expansion layer which maps it to a representation of size $n_H$.
The action encoder, which is a 2-layer fully-connected network, takes as input a 2-dimensional action $a_t$ encoding the car's acceleration and change in steering angle, and also maps it to a representation of size $n_H$.
The representations of the input states, latent variable, and action, which are all now the same size, are combined via addition.
The result is then run through a deconvolutional network with 256-128-64 feature maps, which produces a prediction for the next image $i_{t+1}$, and a 2-layer fully-connected network (with 256 hidden units) which produces a prediction for the next state vector $u_{t+1}$. These are illustrated in Figure \ref{model-components}.


\begin{figure}[t!]
    \centering
    \subfigure[$f_{enc}$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_enc-crop.pdf}} \\
    \subfigure[$f_{dec}$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_dec-crop.pdf}} \\
    \subfigure[$q_\phi$]{\includegraphics[width=0.45\textwidth]{figures/driving/f_phi-crop.pdf}} \\
    \caption{Individual components of the prediction model.}
\end{figure}
\label{model-components}



The specific updates of the stochastic forward model are given by:

\begin{alignat}{2}
  \label{eq:update-eqn}
  &(\mu_\phi, \sigma_\phi) &&= q_\phi(s_{1:t}, s_{t+1}) \\
  &\epsilon &&\sim \mathcal{N}(0, I) \\
  &z_t &&= \mu_\phi + \sigma_\phi \cdot \epsilon \\
  &\hat{s}_{t+1} = (\tilde{i}_{t+1}, \tilde{u}_{t+1}) &&= f_\theta(s_{1:t}, a_t, z_t)
%  &\ell(f_\theta(s_{1:t}, a_t, z_t), s_{t+1}) + \beta D_{\mbox{KL}}(f_\phi(s_{1:t}, s_{t+1}), \mathcal{N}(0, I))
\end{alignat}

The per-sample loss is given by:

\begin{align}
  \label{eq:update-eqn}
  \ell(s_{1:t}, s_{t+1}) = \|\tilde{i}_t - i_t \|_2^2 + \| \tilde{u}_t - u_t \|_2^2 + \beta D_{KL}(\mathcal{N}(\mu_\phi, \sigma_\phi) || p(z))
\end{align}



\section{Planning Approaches}
  \label{planning-methods}

  We now describe different planning approaches using our stochastic forward model, with diagrams shown in Figure \ref{planning-methods}.
  It consists of an encoder with an identical architecture as $f_{enc}$ used in the forward model (which also takes 20 consecutive states as input), followed by a 3-layer fully connected network which outputs a 2-D mean and variance $(\mu, \sigma)$ of a diagonal Gaussian over actions.

  Two of the planning methods optimize a cost function $C$, which is a combination of the proximity and lane costs we described previously, as well as an epistemic uncertainty cost which we describe in the next section. For now, we can treat the cost as a scalar-valued differentiable function of a predicted state.


\begin{figure*}[ht!]
    \centering
    \subfigure[Stochastic Model-Based Imitation Learning (SMBIL)]{\includegraphics[width=0.5\textwidth]{figures/driving/smbil-crop.pdf}} \\
    \label{planning-methods}
    \caption{Planning Methods. SMBIL minimizes the distance between training set trajectories and trajectories predicted by the forward model under the current policy network, using latent variables inferred from the training trajectory. The other methods optimize actions or a policy network to minimize the cost predicted by the forward model, using randomly sampled sequences of latent variables.}
\end{figure*}



  \subsection{Single-Step Imitation Learning}


  The simplest approach to learning a policy from observational data is imitation learning \citep{Pomerleau91}, where a network is trained to predict expert actions from states. Here, we give the network a concatenation of 20 states $s_{1:t}$ as input and train it to minimize the negative log-likelihood of the true action observed in the dataset under the parameters of the distribution output by the model:

    \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ -\mbox{ log } \mathcal{N}(a_{t+1} | \mu, \sigma) \Big],  \mbox{ such that: } (\mu, \sigma) = \pi_\psi(s_{1:t}) \\
  \end{align*}

  \subsection{Stochastic Model-Based Imitation Learning (SMBIL)}

  We also experimented with a variant of imitation learning using the learned stochastic model, which we found performed better than the standard version. One issue with imitation learning is that errors can accumulate quickly, leading to divergence from the states seen during training.
  One cause may be that the model is simply minimizing the $\ell_2$ loss in action space, which may not correspond to minimizing distances in trajectory space.
  Consider the following example where an agent is walking exactly along the side of a cliff, and must output an action which represents its angle.
  To the left is a drop, and to the right is solid ground.
  Say the expert action is to go straight, i.e. $a_{t+1} = 0$. Now consider two possible actions predicted by the network, $\hat{a}_{t+1} = -\epsilon$ (slight left) and $\hat{a}_{t+1} = +\epsilon$ (slight right).
  Both of these actions incur the same $\ell_2$ cost of $\epsilon$, but have very different consequences. If the agent moves slightly away from the expert action on the left side, they fall off the cliff, which causes a large deviation of their subsequent states from the expert states. If they move slightly to the right however, they stay on solid ground and their subsequent states remain close to the expert states.

  As an alternative, we experimented with training a policy to match expert \textit{trajectories}, rather than actions.
  We do this by unrolling the forward model for multiple timesteps, outputting actions by the policy network using the model predictions, and minimizing the error between the final trajectory output by the forward model and the expert trajectory observed in the dataset.
%  This is illustrated in Figure \ref{mbil-diagram}.
  The motivation here is that if the policy network outputs an action which causes small divergence from the target trajectory at the next timestep, but large divergences later on, it will receive gradients from these larger errors backpropagated through the unrolled forward model.

    \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} \ell(s_{t+i}, \hat{s}_{t+1}) \Big],  \mbox{ such that: }
    \begin{cases}
      \hat{s}_t = s_t \\
      z_{t+i} = f_\phi(\hat{s}_{t+i-1}, \hat{s}_{t+i}) \\
      \hat{a}_{t+i} = \pi_\psi(\hat{s}_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}, z_{t+i}) \\
      \end{cases}
  \end{align*}


%  \begin{figure}[t]
%\centering
%\includegraphics[width=0.95\textwidth]{figures/driving/mbil.pdf}
%\caption{Stochastic Model-Based Imitation Learning. The forward model is unrolled for several time steps and latent variables are inferred from the training sequence. Keeping these fixed, the policy network is optimized to produce actions such that the training trajectory is matched.}
%\label{mbil-diagram}
%\end{figure}



  \subsection{Stochastic Value Gradients (SVG)}

  The last approach which we explored was designed to train a policy network using the learned stochastic forward model, using the framework of Stochastic Value Gradients \citep{SVG}.
  We first randomly sample an initial input state $s_t$ from the training set, sample a sequence of latent variables $z$ to represent a future scenario, and then optimize a paramaterized policy network $\pi_\psi$ to minimize the cost predicted by the forward model conditioned on this sequence of latent variables.

    \begin{align*}
    \underset{\psi}{\mbox{ argmin }} \Big[ \sum_{i=1}^{T} C(s_{t+i}) \Big],  \mbox{ such that: }
    \begin{cases}
      z_{t+i} \sim p(z) \\
      \hat{a}_{t+i} = \pi_\psi(s_{t+i-1}) \\
      \hat{s}_{t+i} = f(\hat{s}_{t+i-1}, \hat{a}_{t+i}, z_{t+i}) \\
      \end{cases}
    \end{align*}

    Our approach differs somewhat from the setup of \citep{SVG}, who used latent variables inferred from ground truth trajectories as a means to compensate for model errors. We did not find this to be a problem, possibly because we trained the forward model to make 20-step predictions, whereas they trained the forward model to make single-step predictions.



    \section{Training Details}
    \label{training-details-appendix}

    \subsection{Forward Model}
    We trained our prediction model in deterministic mode ($p=0$) for 200,000 updates, followed by another 200,000 updates in stochastic mode.
    We save the model after training in deterministic mode and treat it as a deterministic baseline.
    Our model was trained using Adam \citep{ADAM} with learning rate 0.0001 and minibatches of size 64, unrolled for 20 time steps, and with dropout ($p_{dropout}=0.1$) at every layer, which was necessary for computing the epistemic uncertainty cost when training the policy network.

    \subsection{Policy Models}

    All policy networks have the same architecture: a 3-layer ConvNet with feature maps of size 64-128-256, followed by 3 fully-connected layers with 256 hidden units each, with the last layer outputting the parameters of a 2D Gaussian distribution over actions. All policy networks are trained with Adam with learning rate 0.0001. The SMBIL and SVG policies are trained by backpropagation through the unrolled forward model using the reparamaterization trick \citep{VAE}. The single-step imitation learner is trained to directly minimize the negative log-likelihood of the ground truth action in the dataset under the parameters output by the policy network. Both SMPC and SVG models unroll the forward model for 20 steps, and we report results for different unrolling lengths for SMBIL policies.


    \section{Action Sensitivity}
    \label{action-sensitivity-appendix}




\end{document}
